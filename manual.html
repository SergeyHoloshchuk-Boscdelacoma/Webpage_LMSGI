<!DOCTYPE html>
<html>

	<head>
		<link rel="stylesheet" href="css/estil.css">
		<title>WhatIsLinux</title>
	</head>
	
	<header>
		<nav id=navmenu>
				<ul class=navwords>
					<li><a href="./index.html" >WhatIsLinux</a></li>
					<li><a href="./manual.html" id=current-page>Man</a></li>
					<li><a href="./philounix.html" class=navwords>Unix Shell</a></li>
					<li><a href="./docs.html" class=navwords>Documentation</a></li>
					<li class=navwords id=extra>Extra
						<ul class=submenu>
							<li><a href="./form.html" class=navwords>Contact</a></li>
						</ul>
					</li>
				</ul>
			</nav>
	</header>
	
	
	<body>
		

		<h1>Man Pages</h1>
		<h2>or: How I Learned to Stop Worrying and Love the Command Line</h2>
		
		 <img src="css/imatges/manpage.jpg" alt="manpage-cat" class="center"> </img>
		
		<p class=quote>"the MANliest way to learn linux"</p>
		<p class=quote2>-Mental Outlaw</p>
		
		<p>When navigating the Linux terminal, the most easiest and effective ways to 
		learn commands as a newbie is by utilizing the manual pages, commonly known as "man pages." These 
		comprehensive resources provide detailed documentation for every command, including its 
		syntax, options, and examples of usage, making them an invaluable tool for both beginners 
		and experienced users.</p>
		
		<p>By simply typing the command:</p>
		
		<p id="manpage" class="terminal">man [command]</p>
		
		<p>you gain instant access to information about the command you introduced</p>
		
		<p>Here are some examples of man pages of the most useful commands you will use in linux:</p>
		
		<p class=quote>sudo</p>
		<p class=terminal>sudo(8) - Linux man page
Name

sudo, sudoedit - execute a command as another user
Synopsis

sudo -h | -K | -k | -V

sudo -v [-AknS] [-g group name | #gid] [-p prompt] [-u user name | #uid]

sudo -l[l] [-AknS] [-g group name | #gid] [-p prompt] [-U user name] [-u user name | #uid] [command]

sudo [-AbEHnPS] [-C fd] [-g group name | #gid] [-p prompt] [-r role] [-t type] [-u user name | #uid] [VAR=value] -i | -s [command]

sudoedit [-AnS] [-C fd] [-g group name | #gid] [-p prompt] [-u user name | #uid] file ...

Description

sudo allows a permitted user to execute a command as the superuser or another user, as specified by the security policy.

sudo supports a plugin architecture for security policies and input/output logging. Third parties can develop and distribute their own policy and I/O logging plugins to work seamlessly with the sudo front end. The default security policy is sudoers, which is configured via the file /etc/sudoers, or via LDAP. See the PLUGINS section for more information.

The security policy determines what privileges, if any, a user has to run sudo. The policy may require that users authenticate themselves with a password or another authentication mechanism. If authentication is required, sudo will exit if the user's password is not entered within a configurable time limit. This limit is policy-specific; the default password prompt timeout for the sudoers security policy is 5 minutes.

Security policies may support credential caching to allow the user to run sudo again for a period of time without requiring authentication. The sudoers policy caches credentials for 5 minutes, unless overridden in sudoers(5). By running sudo with the -v option, a user can update the cached credentials without running a command.

When invoked as sudoedit, the -e option (described below), is implied.

Security policies may log successful and failed attempts to use sudo. If an I/O plugin is configured, the running command's input and output may be logged as well.

The options are as follows:

      -A'            Normally, if sudo requires a password, it will read itfrom the user's terminal.  If the -A (askpass) option isspecified, a (possibly graphical) helper program is executedto read the user's password and output the password to thestandard output.  If the SUDO_ASKPASS environment variable isset, it specifies the path to the helper program.  Otherwise,if /etc/sudo.conf contains a line specifying the askpassprogram, that value will be used.  For example:

    # Path to askpass helper program
    Path askpass /usr/X11R6/bin/ssh-askpass

    If no askpass program is available, sudo will exit with an error. 

      -b'            The -b (background) option tells sudo to run the givencommand in the background.  Note that if you use the -boption you cannot use shell job control to manipulate theprocess.  Most interactive commands will fail to workproperly in background mode.

    -C fd' Normally, sudo will close all open file descriptors other than standard input, standard output and standard error. The -C (close from) option allows the user to specify a starting point above the standard error (file descriptor three). Values less than three are not permitted. The security policy may restrict the user's ability to use the -C option. The sudoers policy only permits use of the -C option when the administrator has enabled the closefrom_override option.

    -E' The -E (preserve environment) option indicates to the security policy that the user wishes to preserve their existing environment variables. The security policy may return an error if the -E option is specified and the user does not have permission to preserve the environment.

    -e' The -e (edit) option indicates that, instead of running a command, the user wishes to edit one or more files. In lieu of a command, the string "sudoedit" is used when consulting the security policy. If the user is authorized by the policy, the following steps are taken:

    1. Temporary copies are made of the files to be edited with the owner set to the invoking user.

    2. The editor specified by the policy is run to edit the temporary files. The sudoers policy uses the SUDO_EDITOR, VISUAL and EDITOR environment variables (in that order). If none of SUDO_EDITOR, VISUAL or EDITOR are set, the first program listed in the editor sudoers(5) option is used.

    3. If they have been modified, the temporary files are copied back to their original location and the temporary versions are removed.

    If the specified file does not exist, it will be created. Note that unlike most commands run by sudo, the editor is run with the invoking user's environment unmodified. If, for some reason, sudo is unable to update a file with its edited version, the user will receive a warning and the edited copy will remain in a temporary file.

    -g group' Normally, sudo runs a command with the primary group set to the one specified by the password database for the user the command is being run as (by default, root). The -g (group) option causes sudo to run the command with the primary group set to group instead. To specify a gid instead of a group name, use #gid. When running commands as a gid, many shells require that the '#' be escaped with a backslash ('\'). If no -u option is specified, the command will be run as the invoking user (not root). In either case, the primary group will be set to group.

    -H' The -H (HOME) option requests that the security policy set the HOME environment variable to the home directory of the target user (root by default) as specified by the password database. Depending on the policy, this may be the default behavior.

    -h' The -h (help) option causes sudo to print a short help message to the standard output and exit.

    -i [command]
    The -i (simulate initial login) option runs the shell specified by the password database entry of the target user as a login shell. This means that login-specific resource files such as .profile or .login will be read by the shell. If a command is specified, it is passed to the shell for execution via the shell's -c option. If no command is specified, an interactive shell is executed. sudo attempts to change to that user's home directory before running the shell. The security policy shall initialize the environment to a minimal set of variables, similar to what is present when a user logs in. The Command Environment section in the sudoers(5) manual documents how the -i option affects the environment in which a command is run when the sudoers policy is in use.

    -K' The -K (sure kill) option is like -k except that it removes the user's cached credentials entirely and may not be used in conjunction with a command or other option. This option does not require a password. Not all security policies support credential caching.

    -k [command]
    When used alone, the -k (kill) option to sudo invalidates the user's cached credentials. The next time sudo is run a password will be required. This option does not require a password and was added to allow a user to revoke sudo permissions from a .logout file. Not all security policies support credential caching.

    When used in conjunction with a command or an option that may require a password, the -k option will cause sudo to ignore the user's cached credentials. As a result, sudo will prompt for a password (if one is required by the security policy) and will not update the user's cached credentials.

    -l[
    l] [command]
    If no command is specified, the -l (list) option will list the allowed (and forbidden) commands for the invoking user (or the user specified by the -U option) on the current host. If a command is specified and is permitted by the security policy, the fully-qualified path to the command is displayed along with any command line arguments. If command is specified but not allowed, sudo will exit with a status value of 1. If the -l option is specified with an l argument (i.e. -ll), or if -l is specified multiple times, a longer list format is used.

    -n' The -n (non-interactive) option prevents sudo from prompting the user for a password. If a password is required for the command to run, sudo will display an error message and exit.

    -P' The -P (preserve group vector) option causes sudo to preserve the invoking user's group vector unaltered. By default, the sudoers policy will initialize the group vector to the list of groups the target user is in. The real and effective group IDs, however, are still set to match the target user.

    -p prompt
    The -p (prompt) option allows you to override the default password prompt and use a custom one. The following percent ('%') escapes are supported by the sudoers policy:

    %H
    expanded to the host name including the domain name (on if the machine's host name is fully qualified or the fqdn option is set in sudoers(5))

    %h
    expanded to the local host name without the domain name

    %p
    expanded to the name of the user whose password is being requested (respects the rootpw, targetpw, and runaspw flags in sudoers(5))

    %U
    expanded to the login name of the user the command will be run as (defaults to root unless the -u option is also specified)

    %u
    expanded to the invoking user's login name

    %%
    two consecutive '%' characters are collapsed into a single '%' character

    The prompt specified by the -p option will override the system password prompt on systems that support PAM unless the passprompt_override flag is disabled in sudoers.

    -r role' The -r (role) option causes the new (SELinux) security context to have the role specified by role.

    -S' The -S (stdin) option causes sudo to read the password from the standard input instead of the terminal device. The password must be followed by a newline character.

    -s [command]
    The -s (shell) option runs the shell specified by the SHELL environment variable if it is set or the shell as specified in the password database. If a command is specified, it is passed to the shell for execution via the shell's -c option. If no command is specified, an interactive shell is executed.

    -t type' The -t (type) option causes the new (SELinux) security context to have the type specified by type. If no type is specified, the default type is derived from the specified role.

    -U user' The -U (other user) option is used in conjunction with the -l option to specify the user whose privileges should be listed. The security policy may restrict listing other users' privileges. The sudoers policy only allows root or a user with the ALL privilege on the current host to use this option.

    -u user' The -u (user) option causes sudo to run the specified command as a user other than root. To specify a uid instead of a user name, #uid. When running commands as a uid, many shells require that the '#' be escaped with a backslash ('\'). Security policies may restrict uids to those listed in the password database. The sudoers policy allows uids that are not in the password database as long as the targetpw option is not set. Other security policies may not support this.

    -V' The -V (version) option causes sudo to print its version string and the version string of the security policy plugin and any I/O plugins. If the invoking user is already root the -V option will display the arguments passed to configure when sudo was built and plugins may display more verbose information such as default options.

    -v' When given the -v (validate) option, sudo will update the user's cached credentials, authenticating the user's password if necessary. For the sudoers plugin, this extends the sudo timeout for another 5 minutes (or whatever the timeout is set to by the security policy) but does not run a command. Not all security policies support cached credentials.

    --' The -- option indicates that sudo should stop processing command line arguments.

    Environment variables to be set for the command may also be passed on the command line in the form of VAR=value, e.g. LD_LIBRARY_PATH=/usr/local/pkg/lib. Variables passed on the command line are subject to the same restrictions as normal environment variables with one important exception. If the setenv option is set in sudoers, the command to be run has the SETENV tag set or the command matched is ALL, the user may set variables that would otherwise be forbidden. See sudoers(5) for more information.
    Command Execution

When sudo executes a command, the security policy specifies the execution envionment for the command. Typically, the real and effective uid and gid are set to match those of the target user, as specified in the password database, and the group vector is initialized based on the group database (unless the -P option was specified).

The following parameters may be specified by security policy:

       â€¢ real and effective user ID

â€¢ real and effective group ID

â€¢ supplementary group IDs

â€¢ the environment list

â€¢ current working directory

â€¢ file creation mode mask (umask)

â€¢ SELinux role and type

â€¢ scheduling priority (aka nice value)

Process model
When sudo runs a command, it calls fork(2), sets up the execution environment as described above, and calls the execve system call in the child process. The main sudo process waits until the command has completed, then passes the command's exit status to the security policy's close method and exits. If an I/O logging plugin is configured, a new pseudo-terminal (''pty'') is created and a second sudo process is used to relay job control signals between the user's existing pty and the new pty the command is being run in. This extra process makes it possible to, for example, suspend and resume the command. Without it, the command would be in what POSIX terms an ''orphaned process group'' and it would not receive any job control signals.

Signal handling
Because the command is run as a child of the sudo process, sudo will relay signals it receives to the command. Unless the command is being run in a new pty, the SIGHUP, SIGINT and SIGQUIT signals are not relayed unless they are sent by a user process, not the kernel. Otherwise, the command would receive SIGINT twice every time the user entered control-C. Some signals, such as SIGSTOP and SIGKILL, cannot be caught and thus will not be relayed to the command. As a general rule, SIGTSTP should be used instead of SIGSTOP when you wish to suspend a command being run by sudo.

As a special case, sudo will not relay signals that were sent by the command it is running. This prevents the command from accidentally killing itself. On some systems, the reboot(8) command sends SIGTERM to all non-system processes other than itself before rebooting the systyem. This prevents sudo from relaying the SIGTERM signal it received back to reboot(8), which might then exit before the system was actually rebooted, leaving it in a half-dead state similar to single user mode. Note, however, that this check only applies to the command run by sudo and not any other processes that the command may create. As a result, running a script that calls reboot(8) or shutdown(8) via sudo may cause the system to end up in this undefined state unless the reboot(8) or shutdown(8) are run using the exec() family of functions instead of system() (which interposes a shell between the command and the calling process).
Plugins

Plugins are dynamically loaded based on the contents of the /etc/sudo.conf file. If no /etc/sudo.conf file is present, or it contains no Plugin lines, sudo will use the traditional sudoers security policy and I/O logging, which corresponds to the following /etc/sudo.conf file.

#
# Default /etc/sudo.conf file
#
# Format:
#   Plugin plugin_name plugin_path plugin_options ...
#   Path askpass /path/to/askpass
#   Path noexec /path/to/sudo_noexec.so
#   Debug sudo /var/log/sudo_debug all@warn
#   Set disable_coredump true
#
# The plugin_path is relative to /usr/libexec unless
#   fully qualified.
# The plugin_name corresponds to a global symbol in the plugin
#   that contains the plugin interface structure.
# The plugin_options are optional.
#
Plugin policy_plugin sudoers.so
Plugin io_plugin sudoers.so

A Plugin line consists of the Plugin keyword, followed by the symbol_name and the path to the shared object containing the plugin. The symbol_name is the name of the struct policy_plugin or struct io_plugin in the plugin shared object. The path may be fully qualified or relative. If not fully qualified it is relative to the /usr/libexec directory. Any additional parameters after the path are passed as arguments to the plugin's open function. Lines that don't begin with Plugin, Path, Debug, or Set are silently ignored.

For more information, see the sudo_plugin(8) manual.
Paths

A Path line consists of the Path keyword, followed by the name of the path to set and its value. E.g.

Path noexec /usr/libexec/sudo_noexec.so
Path askpass /usr/X11R6/bin/ssh-askpass

The following plugin-agnostic paths may be set in the /etc/sudo.conf file:

askpass

    The fully qualified path to a helper program used to read the user's password when no terminal is available. This may be the case when sudo is executed from a graphical (as opposed to text-based) application. The program specified by askpass should display the argument passed to it as the prompt and write the user's password to the standard output. The value of askpass may be overridden by the SUDO_ASKPASS environment variable.

    noexec' The fully-qualified path to a shared library containing dummy versions of the execv(), execve() and fexecve() library functions that just return an error. This is used to implement the noexec functionality on systems that support LD_PRELOAD or its equivalent. Defaults to /usr/libexec/sudo_noexec.so.
    Debug Flags

sudo versions 1.8.4 and higher support a flexible debugging framework that can help track down what sudo is doing internally if there is a problem.

A Debug line consists of the Debug keyword, followed by the name of the program to debug (sudo, visudo, sudoreplay), the debug file name and a comma-separated list of debug flags. The debug flag syntax used by sudo and the sudoers plugin is subsystem@priority but the plugin is free to use a different format so long as it does not include a comma (',').

For instance:

Debug sudo /var/log/sudo_debug all@warn,plugin@info

would log all debugging statements at the warn level and higher in addition to those at the info level for the plugin subsystem.

Currently, only one Debug entry per program is supported. The sudo Debug entry is shared by the sudo front end, sudoedit and the plugins. A future release may add support for per-plugin Debug lines and/or support for multiple debugging files for a single program.

The priorities used by the sudo front end, in order of decreasing severity, are: crit, err, warn, notice, diag, info, trace and debug. Each priority, when specified, also includes all priorities higher than it. For example, a priority of notice would include debug messages logged at notice and higher.

The following subsystems are used by the sudo front-end:

      all'            matches every subsystem

    args' command line argument processing

    conv' user conversation

    edit' sudoedit

    exec' command execution

    main' sudo main function

    netif' network interface handling

    pcomm' communication with the plugin

    plugin' plugin configuration

    pty' pseudo-tty related code

    selinux' SELinux-specific handling

    util' utility functions

    utmp' utmp handling
    Exit Value

Upon successful execution of a program, the exit status from sudo will simply be the exit status of the program that was executed.

Otherwise, sudo exits with a value of 1 if there is a configuration/permission problem or if sudo cannot execute the given command. In the latter case the error string is printed to the standard error. If sudo cannot stat(2) one or more entries in the user's PATH, an error is printed on stderr. (If the directory does not exist or if it is not really a directory, the entry is ignored and no error is printed.) This should not happen under normal circumstances. The most common reason for stat(2) to return ''permission denied'' is if you are running an automounter and one of the directories in your PATH is on a machine that is currently unreachable.
Security Notes

sudo tries to be safe when executing external commands.

To prevent command spoofing, sudo checks "." and "" (both denoting current directory) last when searching for a command in the user's PATH (if one or both are in the PATH). Note, however, that the actual PATH environment variable is not modified and is passed unchanged to the program that sudo executes.

Please note that sudo will normally only log the command it explicitly runs. If a user runs a command such as sudo su or sudo sh, subsequent commands run from that shell are not subject to sudo's security policy. The same is true for commands that offer shell escapes (including most editors). If I/O logging is enabled, subsequent commands will have their input and/or output logged, but there will not be traditional logs for those commands. Because of this, care must be taken when giving users access to commands via sudo to verify that the command does not inadvertently give the user an effective root shell. For more information, please see the PREVENTING SHELL ESCAPES section in sudoers(5).

To prevent the disclosure of potentially sensitive information, sudo disables core dumps by default while it is executing (they are re-enabled for the command that is run). To aid in debugging sudo crashes, you may wish to re-enable core dumps by setting ''disable_coredump'' to false in the /etc/sudo.conf file as follows:

Set disable_coredump false

Note that by default, most operating systems disable core dumps from setuid programs, which includes sudo. To actually get a sudo core file you may need to enable core dumps for setuid processes. On BSD and Linux systems this is accomplished via the sysctl command, on Solaris the coreadm command can be used.
Environment

sudo utilizes the following environment variables. The security policy has control over the actual content of the command's environment.

EDITOR'                 Default editor to use in -e (sudoedit) mode ifneither SUDO_EDITOR nor VISUAL is set.

    MAIL' In -i mode or when env_reset is enabled in sudoers, set to the mail spool of the target user.

    HOME' Set to the home directory of the target user if -i or -H are specified, env_reset or always_set_home are set in sudoers, or when the -s option is specified and set_home is set in sudoers.

    PATH' May be overridden by the security policy.

    SHELL' Used to determine shell to run with -s option.

    SUDO_ASKPASS' Specifies the path to a helper program used to read the password if no terminal is available or if the -A option is specified.

    SUDO_COMMAND' Set to the command run by sudo.

    SUDO_EDITOR' Default editor to use in -e (sudoedit) mode.

    SUDO_GID' Set to the group ID of the user who invoked sudo.

    SUDO_PROMPT' Used as the default password prompt.

    SUDO_PS1' If set, PS1 will be set to its value for the program being run.

    SUDO_UID' Set to the user ID of the user who invoked sudo.

    SUDO_USER' Set to the login name of the user who invoked sudo.

    USER' Set to the target user (root unless the -u option is specified).

    VISUAL' Default editor to use in -e (sudoedit) mode if SUDO_EDITOR is not set.
    Files

/etc/sudo.conf'                          sudo front end configuration

Examples

Note: the following examples assume a properly configured security policy.

To get a file listing of an unreadable directory:

$ sudo ls /usr/local/protected

To list the home directory of user yaz on a machine where the file system holding ~yaz is not exported as root:

$ sudo -u yaz ls ~yaz

To edit the index.html file as user www:

$ sudo -u www vi ~www/htdocs/index.html

To view system logs only accessible to root and users in the adm group:

$ sudo -g adm view /var/log/syslog

To run an editor as jim with a different primary group:

$ sudo -u jim -g audio vi ~jim/sound.txt

To shut down a machine:

$ sudo shutdown -r +15 "quick reboot"

To make a usage listing of the directories in the /home partition. Note that this runs the commands in a sub-shell to make the cd and file redirection work.

$ sudo sh -c "cd /home ; du -s * | sort -rn > USAGE"

See Also

grep(1), su(1), stat(2), passwd(5), sudoers(5), sudo_plugin(8), sudoreplay(8), visudo(8)
History

See the HISTORY file in the sudo distribution (http://www.sudo.ws/sudo/history.html) for a brief history of sudo.
Authors

Many people have worked on sudo over the years; this version consists of code written primarily by:

Todd C. Miller </p>


		<p class=quote>ls</p>
		<p class=terminal>ls(1) - Linux man page
Name
ls - list directory contents
Synopsis
ls [OPTION]... [FILE]...
Description

List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort.

Mandatory arguments to long options are mandatory for short options too.

-a, --all
    do not ignore entries starting with . 
-A, --almost-all
    do not list implied . and .. 
--author
    with -l, print the author of each file 
-b, --escape
    print octal escapes for nongraphic characters 
--block-size=SIZE
    use SIZE-byte blocks. See SIZE format below 
-B, --ignore-backups
    do not list implied entries ending with ~ 
-c
    with -lt: sort by, and show, ctime (time of last modification of file status information) with -l: show ctime and sort by name otherwise: sort by ctime 
-C
    list entries by columns 
--color[=WHEN]
    colorize the output. WHEN defaults to 'always' or can be 'never' or 'auto'. More info below 
-d, --directory
    list directory entries instead of contents, and do not dereference symbolic links 
-D, --dired
    generate output designed for Emacs' dired mode 
-f
    do not sort, enable -aU, disable -ls --color 
-F, --classify
    append indicator (one of */=>@|) to entries 
--file-type
    likewise, except do not append '*' 
--format=WORD
    across -x, commas -m, horizontal -x, long -l, single-column -1, verbose -l, vertical -C 
--full-time
    like -l --time-style=full-iso 
-g
    like -l, but do not list owner 
--group-directories-first
    group directories before files. 
augment with a --sort option, but any
    use of --sort=none (-U) disables grouping 
-G, --no-group
    in a long listing, don't print group names 
-h, --human-readable
    with -l, print sizes in human readable format (e.g., 1K 234M 2G) 
--si
    likewise, but use powers of 1000 not 1024 
-H, --dereference-command-line
    follow symbolic links listed on the command line 
--dereference-command-line-symlink-to-dir
    follow each command line symbolic link that points to a directory 
--hide=PATTERN
    do not list implied entries matching shell PATTERN (overridden by -a or -A) 
--indicator-style=WORD
    append indicator with style WORD to entry names: none (default), slash (-p), file-type (--file-type), classify (-F) 
-i, --inode
    print the index number of each file 
-I, --ignore=PATTERN
    do not list implied entries matching shell PATTERN 
-k
    like --block-size=1K 
-l
    use a long listing format 
-L, --dereference
    when showing file information for a symbolic link, show information for the file the link references rather than for the link itself 
-m
    fill width with a comma separated list of entries 
-n, --numeric-uid-gid
    like -l, but list numeric user and group IDs 
-N, --literal
    print raw entry names (don't treat e.g. control characters specially) 
-o
    like -l, but do not list group information 
-p, --indicator-style=slash
    append / indicator to directories 
-q, --hide-control-chars
    print ? instead of non graphic characters 
--show-control-chars
    show non graphic characters as-is (default unless program is 'ls' and output is a terminal) 
-Q, --quote-name
    enclose entry names in double quotes 
--quoting-style=WORD
    use quoting style WORD for entry names: literal, locale, shell, shell-always, c, escape 
-r, --reverse
    reverse order while sorting 
-R, --recursive
    list subdirectories recursively 
-s, --size
    print the allocated size of each file, in blocks 
-S
    sort by file size 
--sort=WORD
    sort by WORD instead of name: none -U, extension -X, size -S, time -t, version -v 
--time=WORD
    with -l, show time as WORD instead of modification time: atime -u, access -u, use -u, ctime -c, or status -c; use specified time as sort key if --sort=time 
--time-style=STYLE
    with -l, show times using style STYLE: full-iso, long-iso, iso, locale, +FORMAT. FORMAT is interpreted like 'date'; if FORMAT is FORMAT1<newline>FORMAT2, FORMAT1 applies to non-recent files and FORMAT2 to recent files; if STYLE is prefixed with 'posix-', STYLE takes effect only outside the POSIX locale 
-t
    sort by modification time 
-T, --tabsize=COLS
    assume tab stops at each COLS instead of 8 
-u
    with -lt: sort by, and show, access time with -l: show access time and sort by name otherwise: sort by access time 
-U
    do not sort; list entries in directory order 
-v
    natural sort of (version) numbers within text 
-w, --width=COLS
    assume screen width instead of current value 
-x
    list entries by lines instead of by columns 
-X
    sort alphabetically by entry extension 
-1
    list one file per line

SELinux options:

--lcontext
    Display security context. Enable -l. Lines will probably be too wide for most displays. 
-Z, --context
    Display security context so it fits on most displays. Displays only mode, user, group, security context and file name. 
--scontext
    Display only security context and file name. 
--help
    display this help and exit 
--version
    output version information and exit

SIZE may be (or may be an integer optionally followed by) one of following: KB 1000, K 1024, MB 1000*1000, M 1024*1024, and so on for G, T, P, E, Z, Y.

Using color to distinguish file types is disabled both by default and with --color=never. With --color=auto, ls emits color codes only when standard output is connected to a terminal. The LS_COLORS environment variable can change the settings. Use the dircolors command to set it.
Exit status:

    if OK,
    if minor problems (e.g., cannot access subdirectory),
    if serious trouble (e.g., cannot access command-line argument).

Author
Written by Richard M. Stallman and David MacKenzie. </p>

		<p class=quote>pwd</p>
		<p class=terminal>pwd(1) - Linux man page
Name
pwd - print name of current/working directory
Synopsis
pwd [OPTION]...
Description

Print the full filename of the current working directory.

-L, --logical
    use PWD from environment, even if it contains symlinks 
-P, --physical
    avoid all symlinks 
--help
    display this help and exit 
--version
    output version information and exit

NOTE: your shell may have its own version of pwd, which usually supersedes the version described here. Please refer to your shell's documentation for details about the options it supports.
Author
Written by Jim Meyering.</p>

		<p class=quote>cd</p>
		<p class=terminal>cd(1) - Linux man page
Name

bash, :, ., [, alias, bg, bind, break, builtin, caller, cd, command, compgen, complete, compopt, continue, declare, dirs, disown, echo, enable, eval, exec, exit, export, false, fc, fg, getopts, hash, help, history, jobs, kill, let, local, logout, mapfile, popd, printf, pushd, pwd, read, readonly, return, set, shift, shopt, source, suspend, test, times, trap, true, type, typeset, ulimit, umask, unalias, unset, wait - bash built-in commands, see bash(1)
Bash Builtin Commands</p>

		<p class=quote>mkdir</p>
		<p class=terminal>mkdir(1) - Linux man page
Name
mkdir - make directories
Synopsis
mkdir [OPTION]... DIRECTORY...
Description

Create the DIRECTORY(ies), if they do not already exist.

Mandatory arguments to long options are mandatory for short options too.

-m, --mode=MODE
    set file mode (as in chmod), not a=rwx - umask 
-p, --parents
    no error if existing, make parent directories as needed 
-v, --verbose
    print a message for each created directory 
-Z, --context=CTX
    set the SELinux security context of each created directory to CTX 
--help
    display this help and exit 
--version
    output version information and exit

Author
Written by David MacKenzie. </p>

		<p class=quote>rm</p>
		<p class=terminal>rm(1) - Linux man page
Name
rm - remove files or directories
Synopsis
rm [OPTION]... FILE...
Description
This manual page documents the GNU version of rm. rm removes each specified file. By default, it does not remove directories.

If the -I or --interactive=once option is given, and there are more than three files or the -r, -R, or --recursive are given, then rm prompts the user for whether to proceed with the entire operation. If the response is not affirmative, the entire command is aborted.

Otherwise, if a file is unwritable, standard input is a terminal, and the -f or --force option is not given, or the -i or --interactive=always option is given, rm prompts the user for whether to remove the file. If the response is not affirmative, the file is skipped.
Options

Remove (unlink) the FILE(s).

-f, --force
    ignore nonexistent files, never prompt 
-i
    prompt before every removal 
-I
    prompt once before removing more than three files, or when removing recursively. Less intrusive than -i, while still giving protection against most mistakes 
--interactive[=WHEN]
    prompt according to WHEN: never, once (-I), or always (-i). Without WHEN, prompt always 
--one-file-system
    when removing a hierarchy recursively, skip any directory that is on a file system different from that of the corresponding command line argument 
--no-preserve-root
    do not treat '/' specially 
--preserve-root
    do not remove '/' (default) 
-r, -R, --recursive
    remove directories and their contents recursively 
-v, --verbose
    explain what is being done 
--help
    display this help and exit 
--version
    output version information and exit

By default, rm does not remove directories. Use the --recursive (-r or -R) option to remove each listed directory, too, along with all of its contents.

To remove a file whose name starts with a '-', for example '-foo', use one of these commands:

rm -- -foo
rm ./-foo

Note that if you use rm to remove a file, it is usually possible to recover the contents of that file. If you want more assurance that the contents are truly unrecoverable, consider using shred.
Author
Written by Paul Rubin, David MacKenzie, Richard M. Stallman, and Jim Meyering. </p>

		<p class=quote>cp</p>
		<p class=terminal>cp(1) - Linux man page
Name
cp - copy files and directories
Synopsis
cp [OPTION]... [-T] SOURCE DEST
cp [OPTION]... SOURCE... DIRECTORY
cp [OPTION]... -t DIRECTORY SOURCE...
Description

Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.

Mandatory arguments to long options are mandatory for short options too.

-a, --archive
    same as -dR --preserve=all 
--backup[=CONTROL]
    make a backup of each existing destination file 
-b
    like --backup but does not accept an argument 
--copy-contents
    copy contents of special files when recursive 
-d
    same as --no-dereference --preserve=links 
-f, --force
    if an existing destination file cannot be opened, remove it and try again (redundant if the -n option is used) 
-i, --interactive
    prompt before overwrite (overrides a previous -n option) 
-H
    follow command-line symbolic links in SOURCE 
-l, --link
    link files instead of copying 
-L, --dereference
    always follow symbolic links in SOURCE 
-n, --no-clobber
    do not overwrite an existing file (overrides a previous -i option) 
-P, --no-dereference
    never follow symbolic links in SOURCE 
-p
    same as --preserve=mode,ownership,timestamps 
--preserve[=ATTR_LIST]
    preserve the specified attributes (default: mode,ownership,timestamps), if possible additional attributes: context, links, xattr, all 
-c
    same as --preserve=context 
--no-preserve=ATTR_LIST
    don't preserve the specified attributes 
--parents
    use full source file name under DIRECTORY 
-R, -r, --recursive
    copy directories recursively 
--reflink[=WHEN]
    control clone/CoW copies. See below. 
--remove-destination
    remove each existing destination file before attempting to open it (contrast with --force) 
--sparse=WHEN
    control creation of sparse files. See below. 
--strip-trailing-slashes
    remove any trailing slashes from each SOURCE argument 
-s, --symbolic-link
    make symbolic links instead of copying 
-S, --suffix=SUFFIX
    override the usual backup suffix 
-t, --target-directory=DIRECTORY
    copy all SOURCE arguments into DIRECTORY 
-T, --no-target-directory
    treat DEST as a normal file 
-u, --update
    copy only when the SOURCE file is newer than the destination file or when the destination file is missing 
-v, --verbose
    explain what is being done 
-x, --one-file-system
    stay on this file system 
-Z, --context=CONTEXT
    set security context of copy to CONTEXT 
--help
    display this help and exit 
--version
    output version information and exit

By default, sparse SOURCE files are detected by a crude heuristic and the corresponding DEST file is made sparse as well. That is the behavior selected by --sparse=auto. Specify --sparse=always to create a sparse DEST file whenever the SOURCE file contains a long enough sequence of zero bytes. Use --sparse=never to inhibit creation of sparse files.

When --reflink[=always] is specified, perform a lightweight copy, where the data blocks are copied only when modified. If this is not possible the copy fails, or if --reflink=auto is specified, fall back to a standard copy.

The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX. The version control method may be selected via the --backup option or through the VERSION_CONTROL environment variable. Here are the values:

none, off
    never make backups (even if --backup is given) 
numbered, t
    make numbered backups 
existing, nil
    numbered if numbered backups exist, simple otherwise 
simple, never
    always make simple backups

As a special case, cp makes a backup of SOURCE when the force and backup options are given and SOURCE and DEST are the same name for an existing, regular file.
Author
Written by Torbjorn Granlund, David MacKenzie, and Jim Meyering. </p>

		<p class=quote>mv</p>
		<p class=terminal>mv(1) - Linux man page
Name
mv - move (rename) files
Synopsis
mv [OPTION]... [-T] SOURCE DEST
mv [OPTION]... SOURCE... DIRECTORY
mv [OPTION]... -t DIRECTORY SOURCE...
Description

Rename SOURCE to DEST, or move SOURCE(s) to DIRECTORY.

Mandatory arguments to long options are mandatory for short options too.

--backup[=CONTROL]
    make a backup of each existing destination file 
-b
    like --backup but does not accept an argument 
-f, --force
    do not prompt before overwriting 
-i, --interactive
    prompt before overwrite 
-n, --no-clobber
    do not overwrite an existing file

If you specify more than one of -i, -f, -n, only the final one takes effect.

--strip-trailing-slashes
    remove any trailing slashes from each SOURCE argument 
-S, --suffix=SUFFIX
    override the usual backup suffix 
-t, --target-directory=DIRECTORY
    move all SOURCE arguments into DIRECTORY 
-T, --no-target-directory
    treat DEST as a normal file 
-u, --update
    move only when the SOURCE file is newer than the destination file or when the destination file is missing 
-v, --verbose
    explain what is being done 
--help
    display this help and exit 
--version
    output version information and exit

The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX. The version control method may be selected via the --backup option or through the VERSION_CONTROL environment variable. Here are the values:

none, off
    never make backups (even if --backup is given) 
numbered, t
    make numbered backups 
existing, nil
    numbered if numbered backups exist, simple otherwise 
simple, never
    always make simple backups

Author
Written by Mike Parker, David MacKenzie, and Jim Meyering. </p>

		<p class=quote>find</p>
		<p class=terminal>find(1) - Linux man page
Name

find - search for files in a directory hierarchy
Synopsis

find [-H] [-L] [-P] [-D debugopts] [-Olevel] [path...] [expression]
Description

This manual page documents the GNU version of find. GNU find searches the directory tree rooted at each given file name by evaluating the given expression from left to right, according to the rules of precedence (see section OPERATORS), until the outcome is known (the left hand side is false for and operations, true for or), at which point find moves on to the next file name.

If you are using find in an environment where security is important (for example if you are using it to search directories that are writable by other users), you should read the "Security Considerations" chapter of the findutils documentation, which is called Finding Files and comes with findutils. That document also includes a lot more detail and discussion than this manual page, so you may find it a more useful source of information.
Options

The -H, -L and -P options control the treatment of symbolic links. Command-line arguments following these are taken to be names of files or directories to be examined, up to the first argument that begins with '-', or the argument '(' or '!'. That argument and any following arguments are taken to be the expression describing what is to be searched for. If no paths are given, the current directory is used. If no expression is given, the expression -print is used (but you should probably consider using -print0 instead, anyway).

This manual page talks about 'options' within the expression list. These options control the behaviour of find but are specified immediately after the last path name. The five 'real' options -H, -L, -P, -D and -O must appear before the first path name, if at all. A double dash -- can also be used to signal that any remaining arguments are not options (though ensuring that all start points begin with either './' or '/' is generally safer if you use wildcards in the list of start points).

-P

Never follow symbolic links. This is the default behaviour. When find examines or prints information a file, and the file is a symbolic link, the information used shall be taken from the properties of the symbolic link itself.

-L

Follow symbolic links. When find examines or prints information about files, the information used shall be taken from the properties of the file to which the link points, not from the link itself (unless it is a broken symbolic link or find is unable to examine the file to which the link points). Use of this option implies -noleaf. If you later use the -P option, -noleaf will still be in effect. If -L is in effect and find discovers a symbolic link to a subdirectory during its search, the subdirectory pointed to by the symbolic link will be searched.
    When the -L option is in effect, the -type predicate will always match against the type of the file that a symbolic link points to rather than the link itself (unless the symbolic link is broken). Using -L causes the -lname and -ilname predicates always to return false. 
-H

Do not follow symbolic links, except while processing the command line arguments. When find examines or prints information about files, the information used shall be taken from the properties of the symbolic link itself. The only exception to this behaviour is when a file specified on the command line is a symbolic link, and the link can be resolved. For that situation, the information used is taken from whatever the link points to (that is, the link is followed). The information about the link itself is used as a fallback if the file pointed to by the symbolic link cannot be examined. If -H is in effect and one of the paths specified on the command line is a symbolic link to a directory, the contents of that directory will be examined (though of course -maxdepth 0 would prevent this).
If more than one of -H, -L and -P is specified, each overrides the others; the last one appearing on the command line takes effect. Since it is the default, the -P option should be considered to be in effect unless either -H or -L is specified.

GNU find frequently stats files during the processing of the command line itself, before any searching has begun. These options also affect how those arguments are processed. Specifically, there are a number of tests that compare files listed on the command line against a file we are currently considering. In each case, the file specified on the command line will have been examined and some of its properties will have been saved. If the named file is in fact a symbolic link, and the -P option is in effect (or if neither -H nor -L were specified), the information used for the comparison will be taken from the properties of the symbolic link. Otherwise, it will be taken from the properties of the file the link points to. If find cannot follow the link (for example because it has insufficient privileges or the link points to a nonexistent file) the properties of the link itself will be used.

When the -H or -L options are in effect, any symbolic links listed as the argument of -newer will be dereferenced, and the timestamp will be taken from the file to which the symbolic link points. The same consideration applies to -newerXY, -anewer and -cnewer.

The -follow option has a similar effect to -L, though it takes effect at the point where it appears (that is, if -L is not used but -follow is, any symbolic links appearing after -follow on the command line will be dereferenced, and those before it will not).
-D debugoptions
    Print diagnostic information; this can be helpful to diagnose problems with why find is not doing what you want. The list of debug options should be comma separated. Compatibility of the debug options is not guaranteed between releases of findutils. For a complete list of valid debug options, see the output of find -D help. Valid debug options include 
    help

    Explain the debugging options

    tree

    Show the expression tree in its original and optimised form.

    stat

    Print messages as files are examined with the stat and lstat system calls. The find program tries to minimise such calls.

    opt

    Prints diagnostic information relating to the optimisation of the expression tree; see the -O option.

    rates

    Prints a summary indicating how often each predicate succeeded or failed. 
-Olevel
    Enables query optimisation. The find program reorders tests to speed up execution while preserving the overall effect; that is, predicates with side effects are not reordered relative to each other. The optimisations performed at each optimisation level are as follows. 
    0

    Equivalent to optimisation level 1.

    1

    This is the default optimisation level and corresponds to the traditional behaviour. Expressions are reordered so that tests based only on the names of files (for example -name and -regex) are performed first.

    2

    Any -type or -xtype tests are performed after any tests based only on the names of files, but before any tests that require information from the inode. On many modern versions of Unix, file types are returned by readdir() and so these predicates are faster to evaluate than predicates which need to stat the file first.

    3

    At this optimisation level, the full cost-based query optimiser is enabled. The order of tests is modified so that cheap (i.e. fast) tests are performed first and more expensive ones are performed later, if necessary. Within each cost band, predicates are evaluated earlier or later according to whether they are likely to succeed or not. For -o, predicates which are likely to succeed are evaluated earlier, and for -a, predicates which are likely to fail are evaluated earlier. 
    The cost-based optimiser has a fixed idea of how likely any given test is to succeed. In some cases the probability takes account of the specific nature of the test (for example, -type f is assumed to be more likely to succeed than -type c). The cost-based optimiser is currently being evaluated. If it does not actually improve the performance of find, it will be removed again. Conversely, optimisations that prove to be reliable, robust and effective may be enabled at lower optimisation levels over time. However, the default behaviour (i.e. optimisation level 1) will not be changed in the 4.3.x release series. The findutils test suite runs all the tests on find at each optimisation level and ensures that the result is the same. 

Expressions

The expression is made up of options (which affect overall operation rather than the processing of a specific file, and always return true), tests (which return a true or false value), and actions (which have side effects and return a true or false value), all separated by operators. -and is assumed where the operator is omitted.

If the expression contains no actions other than -prune, -print is performed on all files for which the expression is true.

OPTIONS

All options always return true. Except for -daystart, -follow and -regextype, the options affect all tests, including tests specified before the option. This is because the options are processed when the command line is parsed, while the tests don't do anything until files are examined. The -daystart, -follow and -regextype options are different in this respect, and have an effect only on tests which appear later in the command line. Therefore, for clarity, it is best to place them at the beginning of the expression. A warning is issued if you don't do this.
-d

A synonym for -depth, for compatibility with FreeBSD, NetBSD, MacOS X and OpenBSD.
-daystart
    Measure times (for -amin, -atime, -cmin, -ctime, -mmin, and -mtime) from the beginning of today rather than from 24 hours ago. This option only affects tests which appear later on the command line. 
-depth

Process each directory's contents before the directory itself. The -delete action also implies -depth.
-follow
    Deprecated; use the -L option instead. Dereference symbolic links. Implies -noleaf. The -follow option affects only those tests which appear after it on the command line. Unless the -H or -L option has been specified, the position of the -follow option changes the behaviour of the -newer predicate; any files listed as the argument of -newer will be dereferenced if they are symbolic links. The same consideration applies to -newerXY, -anewer and -cnewer. Similarly, the -type predicate will always match against the type of the file that a symbolic link points to rather than the link itself. Using -follow causes the -lname and -ilname predicates always to return false. 
-help, --help
    Print a summary of the command-line usage of find and exit. 
-ignore_readdir_race
    Normally, find will emit an error message when it fails to stat a file. If you give this option and a file is deleted between the time find reads the name of the file from the directory and the time it tries to stat the file, no error message will be issued. This also applies to files or directories whose names are given on the command line. This option takes effect at the time the command line is read, which means that you cannot search one part of the filesystem with this option on and part of it with this option off (if you need to do that, you will need to issue two find commands instead, one with the option and one without it). 
-maxdepth levels
    Descend at most levels (a non-negative integer) levels of directories below the command line arguments. -maxdepth 0 means only apply the tests and actions to the command line arguments. 
-mindepth levels
    Do not apply any tests or actions at levels less than levels (a non-negative integer). -mindepth 1 means process all files except the command line arguments. 
-mount

Don't descend directories on other filesystems. An alternate name for -xdev, for compatibility with some other versions of find.
-noignore_readdir_race
    Turns off the effect of -ignore_readdir_race. 
-noleaf
    Do not optimize by assuming that directories contain 2 fewer subdirectories than their hard link count. This option is needed when searching filesystems that do not follow the Unix directory-link convention, such as CD-ROM or MS-DOS filesystems or AFS volume mount points. Each directory on a normal Unix filesystem has at least 2 hard links: its name and its '.' entry. Additionally, its subdirectories (if any) each have a '..' entry linked to that directory. When find is examining a directory, after it has statted 2 fewer subdirectories than the directory's link count, it knows that the rest of the entries in the directory are non-directories ('leaf' files in the directory tree). If only the files' names need to be examined, there is no need to stat them; this gives a significant increase in search speed. 
-regextype type
    Changes the regular expression syntax understood by -regex and -iregex tests which occur later on the command line. Currently-implemented types are emacs (this is the default), posix-awk, posix-basic, posix-egrep and posix-extended. 
-version, --version
    Print the find version number and exit. 
-warn, -nowarn
    Turn warning messages on or off. These warnings apply only to the command line usage, not to any conditions that find might encounter when it searches directories. The default behaviour corresponds to -warn if standard input is a tty, and to -nowarn otherwise. 
-xautofs
    Don't descend directories on autofs filesystems. 
-xdev

Don't descend directories on other filesystems.

TESTS

Some tests, for example -newerXY and -samefile, allow comparison between the file currently being examined and some reference file specified on the command line. When these tests are used, the interpretation of the reference file is determined by the options -H, -L and -P and any previous -follow, but the reference file is only examined once, at the time the command line is parsed. If the reference file cannot be examined (for example, the stat(2) system call fails for it), an error message is issued, and find exits with a nonzero status.

Numeric arguments can be specified as
+n

for greater than n,

-n

for less than n,

n

for exactly n.
-amin n
    File was last accessed n minutes ago. 
-anewer file
    File was last accessed more recently than file was modified. If file is a symbolic link and the -H option or the -L option is in effect, the access time of the file it points to is always used. 
-atime n
    File was last accessed n*24 hours ago. When find figures out how many 24-hour periods ago the file was last accessed, any fractional part is ignored, so to match -atime +1, a file has to have been accessed at least two days ago. 
-cmin n
    File's status was last changed n minutes ago. 
-cnewer file
    File's status was last changed more recently than file was modified. If file is a symbolic link and the -H option or the -L option is in effect, the status-change time of the file it points to is always used. 
-ctime n
    File's status was last changed n*24 hours ago. See the comments for -atime to understand how rounding affects the interpretation of file status change times. 
-empty

File is empty and is either a regular file or a directory.
-executable
    Matches files which are executable and directories which are searchable (in a file name resolution sense). This takes into account access control lists and other permissions artefacts which the -perm test ignores. This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so cannot make use of the UID mapping information held on the server. Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed. 
-false

Always false.
-fstype type
    File is on a filesystem of type type. The valid filesystem types vary among different versions of Unix; an incomplete list of filesystem types that are accepted on some version of Unix or another is: ufs, 4.2, 4.3, nfs, tmp, mfs, S51K, S52K. You can use -printf with the %F directive to see the types of your filesystems. 
-gid n

File's numeric group ID is n.
-group gname
    File belongs to group gname (numeric group ID allowed). 
-ilname pattern
    Like -lname, but the match is case insensitive. If the -L option or the -follow option is in effect, this test returns false unless the symbolic link is broken. 
-iname pattern
    Like -name, but the match is case insensitive. For example, the patterns 'fo*' and 'F??' match the file names 'Foo', 'FOO', 'foo', 'fOo', etc. In these patterns, unlike filename expansion by the shell, an initial '.' can be matched by '*'. That is, find -name *bar will match the file '.foobar'. Please note that you should quote patterns as a matter of course, otherwise the shell will expand any wildcard characters in them. 
-inum n
    File has inode number n. It is normally easier to use the -samefile test instead. 
-ipath pattern
    Behaves in the same way as -iwholename. This option is deprecated, so please do not use it. 
-iregex pattern
    Like -regex, but the match is case insensitive. 
-iwholename pattern
    Like -wholename, but the match is case insensitive. 
-links n
    File has n links. 
-lname pattern
    File is a symbolic link whose contents match shell pattern pattern. The metacharacters do not treat '/' or '.' specially. If the -L option or the -follow option is in effect, this test returns false unless the symbolic link is broken. 
-mmin n
    File's data was last modified n minutes ago. 
-mtime n
    File's data was last modified n*24 hours ago. See the comments for -atime to understand how rounding affects the interpretation of file modification times. 
-name pattern
    Base of file name (the path with the leading directories removed) matches shell pattern pattern. The metacharacters ('*', '?', and '[]') match a '.' at the start of the base name (this is a change in findutils-4.2.2; see section STANDARDS CONFORMANCE below). To ignore a directory and the files under it, use -prune; see an example in the description of -path. Braces are not recognised as being special, despite the fact that some shells including Bash imbue braces with a special meaning in shell patterns. The filename matching is performed with the use of the fnmatch(3) library function. Don't forget to enclose the pattern in quotes in order to protect it from expansion by the shell. 
-newer file
    File was modified more recently than file. If file is a symbolic link and the -H option or the -L option is in effect, the modification time of the file it points to is always used. 
-newerXY reference
    Compares the timestamp of the current file with reference. The reference argument is normally the name of a file (and one of its timestamps is used for the comparison) but it may also be a string describing an absolute time. X and Y are placeholders for other letters, and these letters select which time belonging to how reference is used for the comparison. 
    Some combinations are invalid; for example, it is invalid for X to be t. Some combinations are not implemented on all systems; for example B is not supported on all systems. If an invalid or unsupported combination of XY is specified, a fatal error results. Time specifications are interpreted as for the argument to the -d option of GNU date. If you try to use the birth time of a reference file, and the birth time cannot be determined, a fatal error message results. If you specify a test which refers to the birth time of files being examined, this test will fail for any files where the birth time is unknown.

    -nogroup

    No group corresponds to file's numeric group ID. 
-nouser
    No user corresponds to file's numeric user ID. 
-path pattern
    File name matches shell pattern pattern. The metacharacters do not treat '/' or '.' specially; so, for example, 

find . -path "./sr*sc"

    will print an entry for a directory called './src/misc' (if one exists). To ignore a whole directory tree, use -prune rather than checking every file in the tree. For example, to skip the directory 'src/emacs' and all files and directories under it, and print the names of the other files found, do something like this: 

find . -path ./src/emacs -prune -o -print

    Note that the pattern match test applies to the whole file name, starting from one of the start points named on the command line. It would only make sense to use an absolute path name here if the relevant start point is also an absolute path. This means that this command will never match anything: 

find bar -path /foo/bar/myfile -print

    The predicate -path is also supported by HP-UX find and will be in a forthcoming version of the POSIX standard. 
-perm mode
    File's permission bits are exactly mode (octal or symbolic). Since an exact match is required, if you want to use this form for symbolic modes, you may have to specify a rather complex mode string. For example -perm g=w will only match files which have mode 0020 (that is, ones for which group write permission is the only permission set). It is more likely that you will want to use the '/' or '-' forms, for example -perm -g=w, which matches any file with group write permission. See the EXAMPLES section for some illustrative examples. 
-perm -mode
    All of the permission bits mode are set for the file. Symbolic modes are accepted in this form, and this is usually the way in which would want to use them. You must specify 'u', 'g' or 'o' if you use a symbolic mode. See the EXAMPLES section for some illustrative examples. 
-perm /mode
    Any of the permission bits mode are set for the file. Symbolic modes are accepted in this form. You must specify 'u', 'g' or 'o' if you use a symbolic mode. See the EXAMPLES section for some illustrative examples. If no permission bits in mode are set, this test matches any file (the idea here is to be consistent with the behaviour of -perm -000). 
-perm +mode
    Deprecated, old way of searching for files with any of the permission bits in mode set. You should use -perm /mode instead. Trying to use the '+' syntax with symbolic modes will yield surprising results. For example, '+u+x' is a valid symbolic mode (equivalent to +u,+x, i.e. 0111) and will therefore not be evaluated as -perm +mode but instead as the exact mode specifier -perm mode and so it matches files with exact permissions 0111 instead of files with any execute bit set. If you found this paragraph confusing, you're not alone - just use -perm /mode. This form of the -perm test is deprecated because the POSIX specification requires the interpretation of a leading '+' as being part of a symbolic mode, and so we switched to using '/' instead. 
-readable
    Matches files which are readable. This takes into account access control lists and other permissions artefacts which the -perm test ignores. This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so cannot make use of the UID mapping information held on the server. 
-regex pattern
    File name matches regular expression pattern. This is a match on the whole path, not a search. For example, to match a file named './fubar3', you can use the regular expression '.*bar.' or '.*b.*3', but not 'f.*r3'. The regular expressions understood by find are by default Emacs Regular Expressions, but this can be changed with the -regextype option. 
-samefile name
    File refers to the same inode as name. When -L is in effect, this can include symbolic links. 
-size n[cwbkMG]
    File uses n units of space. The following suffixes can be used: 
    'b'

    for 512-byte blocks (this is the default if no suffix is used)

    'c'

    for bytes

    'w'

    for two-byte words

    'k'

    for Kilobytes (units of 1024 bytes)

    'M'

    for Megabytes (units of 1048576 bytes)

    'G'

    for Gigabytes (units of 1073741824 bytes) 
    The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated. Bear in mind that the '%k' and '%b' format specifiers of -printf handle sparse files differently. The 'b' suffix always denotes 512-byte blocks and never 1 Kilobyte blocks, which is different to the behaviour of -ls. 
-true

Always true.
-type c
    File is of type c: 
b

block (buffered) special

c

character (unbuffered) special

d

directory

p

named pipe (FIFO)

f

regular file

l

symbolic link; this is never true if the -L option or the -follow option is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype.

s

socket

D

door (Solaris)

-uid n

File's numeric user ID is n.
-used n
    File was last accessed n days after its status was last changed. 
-user uname
    File is owned by user uname (numeric user ID allowed). 
-wholename pattern
    See -path. This alternative is less portable than -path. 
-writable
    Matches files which are writable. This takes into account access control lists and other permissions artefacts which the -perm test ignores. This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so cannot make use of the UID mapping information held on the server. 
-xtype c
    The same as -type unless the file is a symbolic link. For symbolic links: if the -H or -P option was specified, true if the file is a link to a file of type c; if the -L option has been given, true if c is 'l'. In other words, for symbolic links, -xtype checks the type of the file that -type does not check. 
-context pattern
    (SELinux only) Security context of the file matches glob pattern. 

ACTIONS

-delete
    Delete files; true if removal succeeded. If the removal failed, an error message is issued. If -delete fails, find's exit status will be nonzero (when it eventually exits). Use of -delete automatically turns on the -depth option.

    Warnings: Don't forget that the find command line is evaluated as an expression, so putting -delete first will make find try to delete everything below the starting points you specified. When testing a find command line that you later intend to use with -delete, you should explicitly specify -depth in order to avoid later surprises. Because -delete implies -depth, you cannot usefully use -prune and -delete together. 
-exec command ;
    Execute command; true if 0 status is returned. All following arguments to find are taken to be arguments to the command until an argument consisting of ';' is encountered. The string '{}' is replaced by the current file name being processed everywhere it occurs in the arguments to the command, not just in arguments where it is alone, as in some versions of find. Both of these constructions might need to be escaped (with a '\') or quoted to protect them from expansion by the shell. See the EXAMPLES section for examples of the use of the -exec option. The specified command is run once for each matched file. The command is executed in the starting directory. There are unavoidable security problems surrounding use of the -exec action; you should use the -execdir option instead. 
-exec command {} +
    This variant of the -exec action runs the specified command on the selected files, but the command line is built by appending each selected file name at the end; the total number of invocations of the command will be much less than the number of matched files. The command line is built in much the same way that xargs builds its command lines. Only one instance of '{}' is allowed within the command. The command is executed in the starting directory. 
-execdir command ;
-execdir command {} +
    Like -exec, but the specified command is run from the subdirectory containing the matched file, which is not normally the directory in which you started find. This a much more secure method for invoking commands, as it avoids race conditions during resolution of the paths to the matched files. As with the -exec action, the '+' form of -execdir will build a command line to process more than one matched file, but any given invocation of command will only list files that exist in the same subdirectory. If you use this option, you must ensure that your $PATH environment variable does not reference '.'; otherwise, an attacker can run any commands they like by leaving an appropriately-named file in a directory in which you will run -execdir. The same applies to having entries in $PATH which are empty or which are not absolute directory names. 
-fls file
    True; like -ls but write to file like -fprint. The output file is always created, even if the predicate is never matched. See the UNUSUAL FILENAMES section for information about how unusual characters in filenames are handled. 
-fprint file
    True; print the full file name into file file. If file does not exist when find is run, it is created; if it does exist, it is truncated. The file names ''/dev/stdout'' and ''/dev/stderr'' are handled specially; they refer to the standard output and standard error output, respectively. The output file is always created, even if the predicate is never matched. See the UNUSUAL FILENAMES section for information about how unusual characters in filenames are handled. 
-fprint0 file
    True; like -print0 but write to file like -fprint. The output file is always created, even if the predicate is never matched. See the UNUSUAL FILENAMES section for information about how unusual characters in filenames are handled. 
-fprintf file format
    True; like -printf but write to file like -fprint. The output file is always created, even if the predicate is never matched. See the UNUSUAL FILENAMES section for information about how unusual characters in filenames are handled. 
-ls

True; list current file in ls -dils format on standard output. The block counts are of 1K blocks, unless the environment variable POSIXLY_CORRECT is set, in which case 512-byte blocks are used. See the UNUSUAL FILENAMES section for information about how unusual characters in filenames are handled.
-ok command ;
    Like -exec but ask the user first. If the user agrees, run the command. Otherwise just return false. If the command is run, its standard input is redirected from /dev/null. 
    The response to the prompt is matched against a pair of regular expressions to determine if it is an affirmative or negative response. This regular expression is obtained from the system if the 'POSIXLY_CORRECT' environment variable is set, or otherwise from find's message translations. If the system has no suitable definition, find's own definition will be used. In either case, the interpretation of the regular expression itself will be affected by the environment variables 'LC_CTYPE' (character classes) and 'LC_COLLATE' (character ranges and equivalence classes). 
-okdir command ;
    Like -execdir but ask the user first in the same way as for -ok. If the user does not agree, just return false. If the command is run, its standard input is redirected from /dev/null. 
-print

True; print the full file name on the standard output, followed by a newline. If you are piping the output of find into another program and there is the faintest possibility that the files which you are searching for might contain a newline, then you should seriously consider using the -print0 option instead of -print. See the UNUSUAL FILENAMES section for information about how unusual characters in filenames are handled.
-print0
    True; print the full file name on the standard output, followed by a null character (instead of the newline character that -print uses). This allows file names that contain newlines or other types of white space to be correctly interpreted by programs that process the find output. This option corresponds to the -0 option of xargs. 
-printf format
    True; print format on the standard output, interpreting '\' escapes and '%' directives. Field widths and precisions can be specified as with the 'printf' C function. Please note that many of the fields are printed as %s rather than %d, and this may mean that flags don't work as you might expect. This also means that the '-' flag does work (it forces fields to be left-aligned). Unlike -print, -printf does not add a newline at the end of the string. The escapes and directives are: 
    \a

    Alarm bell.

    \b

    Backspace.

    \c

    Stop printing from this format immediately and flush the output.

    \f

    Form feed.

    \n

    Newline.

    \r

    Carriage return.

    \t

    Horizontal tab.

    \v

    Vertical tab.

    \0

    ASCII NUL.

    \\

    A literal backslash ('\').

    \NNN

    The character whose ASCII code is NNN (octal). 
    A '\' character followed by any other character is treated as an ordinary character, so they both are printed. 
    %%

    A literal percent sign.

    %a

    File's last access time in the format returned by the C 'ctime' function.

    %Ak

    File's last access time in the format specified by k, which is either '@' or a directive for the C 'strftime' function. The possible values for k are listed below; some of them might not be available on all systems, due to differences in 'strftime' between systems. 

@

seconds since Jan. 1, 1970, 00:00 GMT, with fractional part.

Time fields:

H

hour (00..23)

I

hour (01..12)

k

hour ( 0..23)

l

hour ( 1..12)

M

minute (00..59)

p

locale's AM or PM

r

time, 12-hour (hh:mm:ss [AP]M)

S

Second (00.00 .. 61.00). There is a fractional part.

T

time, 24-hour (hh:mm:ss)

+

Date and time, separated by '+', for example '2004-04-28+22:22:05.0'. This is a GNU extension. The time is given in the current timezone (which may be affected by setting the TZ environment variable). The seconds field includes a fractional part.

X

locale's time representation (H:M:S)

Z

time zone (e.g., EDT), or nothing if no time zone is determinable

Date fields:

    a

    locale's abbreviated weekday name (Sun..Sat)

    A

    locale's full weekday name, variable length (Sunday..Saturday)

    b

    locale's abbreviated month name (Jan..Dec)

    B

    locale's full month name, variable length (January..December)

    c

    locale's date and time (Sat Nov 04 12:02:33 EST 1989). The format is the same as for ctime(3) and so to preserve compatibility with that format, there is no fractional part in the seconds field.

    d

    day of month (01..31)

    D

    date (mm/dd/yy)

    h

    same as b

    j

    day of year (001..366)

    m

    month (01..12)

    U

    week number of year with Sunday as first day of week (00..53)

    w

    day of week (0..6)

    W

    week number of year with Monday as first day of week (00..53)

    x

    locale's date representation (mm/dd/yy)

    y

    last two digits of year (00..99)

    Y

    year (1970...)

    %b

    The amount of disk space used for this file in 
    512-byte blocks. Since disk space is allocated in multiples of the filesystem block size this is usually greater than %s/512, but it can also be smaller if the file is a sparse file. 
    %c

    File's last status change time in the format returned by the C 'ctime' function.

    %Ck

    File's last status change time in the format specified by k, which is the same as for %A.

    %d

    File's depth in the directory tree; 0 means the file is a command line argument.

    %D

    The device number on which the file exists (the st_dev field of struct stat), in decimal.

    %f

    File's name with any leading directories removed (only the last element).

    %F

    Type of the filesystem the file is on; this value can be used for -fstype.

    %g

    File's group name, or numeric group ID if the group has no name.

    %G

    File's numeric group ID.

    %h

    Leading directories of file's name (all but the last element). If the file name contains no slashes (since it is in the current directory) the %h specifier expands to ".".

    %H

    Command line argument under which file was found.

    %i

    File's inode number (in decimal).

    %k

    The amount of disk space used for this file in 1K blocks. Since disk space is allocated in multiples of the filesystem block size this is usually greater than %s/1024, but it can also be smaller if the file is a sparse file.

    %l

    Object of symbolic link (empty string if file is not a symbolic link).

    %m

    File's permission bits (in octal). This option uses the 'traditional' numbers which most Unix implementations use, but if your particular implementation uses an unusual ordering of octal permissions bits, you will see a difference between the actual value of the file's mode and the output of %m. Normally you will want to have a leading zero on this number, and to do this, you should use the # flag (as in, for example, '%#m').

    %M

    File's permissions (in symbolic form, as for ls). This directive is supported in findutils 4.2.5 and later.

    %n

    Number of hard links to file.

    %p

    File's name.

    %P

    File's name with the name of the command line argument under which it was found removed.

    %s

    File's size in bytes.

    %S

    File's sparseness. This is calculated as (BLOCKSIZE*st_blocks / st_size). The exact value you will get for an ordinary file of a certain length is system-dependent. However, normally sparse files will have values less than 1.0, and files which use indirect blocks may have a value which is greater than 1.0. The value used for BLOCKSIZE is system-dependent, but is usually 512 bytes. If the file size is zero, the value printed is undefined. On systems which lack support for st_blocks, a file's sparseness is assumed to be 1.0.

    %t

    File's last modification time in the format returned by the C 'ctime' function.

    %Tk

    File's last modification time in the format specified by k, which is the same as for %A.

    %u

    File's user name, or numeric user ID if the user has no name.

    %U

    File's numeric user ID.

    %y

    File's type (like in ls -l), U=unknown type (shouldn't happen)

    %Y

    File's type (like %y), plus follow symlinks: L=loop, N=nonexistent

    %Z

    (SELinux only) file's security context. 
    A '%' character followed by any other character is discarded, but the other character is printed (don't rely on this, as further format characters may be introduced). A '%' at the end of the format argument causes undefined behaviour since there is no following character. In some locales, it may hide your door keys, while in others it may remove the final page from the novel you are reading.

    The %m and %d directives support the # , 0 and + flags, but the other directives do not, even if they print numbers. Numeric directives that do not support these flags include G, U, b, D, k and n. The '-' format flag is supported and changes the alignment of a field from right-justified (which is the default) to left-justified.

    See the UNUSUAL FILENAMES section for information about how unusual characters in filenames are handled. 
-prune

True; if the file is a directory, do not descend into it. If -depth is given, false; no effect. Because -delete implies -depth, you cannot usefully use -prune and -delete together.

-quit

Exit immediately. No child processes will be left running, but no more paths specified on the command line will be processed. For example, find /tmp/foo /tmp/bar -print -quit will print only /tmp/foo. Any command lines which have been built up with -execdir ... {} + will be invoked before find exits. The exit status may or may not be zero, depending on whether an error has already occurred.

UNUSUAL FILENAMES

Many of the actions of find result in the printing of data which is under the control of other users. This includes file names, sizes, modification times and so forth. File names are a potential problem since they can contain any character except '\0' and '/'. Unusual characters in file names can do unexpected and often undesirable things to your terminal (for example, changing the settings of your function keys on some terminals). Unusual characters are handled differently by various actions, as described below.
-print0, -fprint0
    Always print the exact filename, unchanged, even if the output is going to a terminal. 
-ls, -fls
    Unusual characters are always escaped. White space, backslash, and double quote characters are printed using C-style escaping (for example '\f', '\"'). Other unusual characters are printed using an octal escape. Other printable characters (for -ls and -fls these are the characters between octal 041 and 0176) are printed as-is. 
-printf, -fprintf
    If the output is not going to a terminal, it is printed as-is. Otherwise, the result depends on which directive is in use. The directives %D, %F, %g, %G, %H, %Y, and %y expand to values which are not under control of files' owners, and so are printed as-is. The directives %a, %b, %c, %d, %i, %k, %m, %M, %n, %s, %t, %u and %U have values which are under the control of files' owners but which cannot be used to send arbitrary data to the terminal, and so these are printed as-is. The directives %f, %h, %l, %p and %P are quoted. This quoting is performed in the same way as for GNU ls. This is not the same quoting mechanism as the one used for -ls and -fls. If you are able to decide what format to use for the output of find then it is normally better to use '\0' as a terminator than to use newline, as file names can contain white space and newline characters. The setting of the 'LC_CTYPE' environment variable is used to determine which characters need to be quoted. 
-print, -fprint
    Quoting is handled in the same way as for -printf and -fprintf. If you are using find in a script or in a situation where the matched files might have arbitrary names, you should consider using -print0 instead of -print. 
The -ok and -okdir actions print the current filename as-is. This may change in a future release.

OPERATORS

Listed in order of decreasing precedence:
( expr )
    Force precedence. Since parentheses are special to the shell, you will normally need to quote them. Many of the examples in this manual page use backslashes for this purpose: '\(...\)' instead of '(...)'. 
! expr

True if expr is false. This character will also usually need protection from interpretation by the shell.
-not expr
    Same as ! expr, but not POSIX compliant. 
expr1 expr2
    Two expressions in a row are taken to be joined with an implied "and"; expr2 is not evaluated if expr1 is false. 
expr1 -a expr2
    Same as expr1 expr2. 
expr1 -and expr2
    Same as expr1 expr2, but not POSIX compliant. 
expr1 -o expr2
    Or; expr2 is not evaluated if expr1 is true. 
expr1 -or expr2
    Same as expr1 -o expr2, but not POSIX compliant. 
expr1 , expr2
    List; both expr1 and expr2 are always evaluated. The value of expr1 is discarded; the value of the list is the value of expr2. The comma operator can be useful for searching for several different types of thing, but traversing the filesystem hierarchy only once. The -fprintf action can be used to list the various matched items into several different output files. 

Standards Conformance

For closest compliance to the POSIX standard, you should set the POSIXLY_CORRECT environment variable. The following options are specified in the POSIX standard (IEEE Std 1003.1, 2003 Edition):

-H

This option is supported.

-L

This option is supported.

-name

This option is supported, but POSIX conformance depends on the POSIX conformance of the system's fnmatch(3) library function. As of findutils-4.2.2, shell metacharacters ('*', '?' or '[]' for example) will match a leading '.', because IEEE PASC interpretation 126 requires this. This is a change from previous versions of findutils.

-type

Supported. POSIX specifies 'b', 'c', 'd', 'l', 'p', 'f' and 's'. GNU find also supports 'D', representing a Door, where the OS provides these.

-ok

Supported. Interpretation of the response is according to the "yes" and "no" patterns selected by setting the 'LC_MESSAGES' environment variable. When the 'POSIXLY_CORRECT' environment variable is set, these patterns are taken system's definition of a positive (yes) or negative (no) response. See the system's documentation for nl_langinfo(3), in particular YESEXPR and NOEXPR. When 'POSIXLY_CORRECT' is not set, the patterns are instead taken from find's own message catalogue.

-newer

Supported. If the file specified is a symbolic link, it is always dereferenced. This is a change from previous behaviour, which used to take the relevant time from the symbolic link; see the HISTORY section below.

-perm

Supported. If the POSIXLY_CORRECT environment variable is not set, some mode arguments (for example +a+x) which are not valid in POSIX are supported for backward-compatibility.
Other predicates
    The predicates -atime, -ctime, -depth, -group, -links, -mtime, -nogroup, -nouser, -print, -prune, -size, -user and -xdev are all supported. 
The POSIX standard specifies parentheses '(', ')', negation '!' and the 'and' and 'or' operators ( -a, -o).

All other options, predicates, expressions and so forth are extensions beyond the POSIX standard. Many of these extensions are not unique to GNU find, however.

The POSIX standard requires that find detects loops:
    The find utility shall detect infinite loops; that is, entering a previously visited directory that is an ancestor of the last file encountered. When it detects an infinite loop, find shall write a diagnostic message to standard error and shall either recover its position in the hierarchy or terminate. 
GNU find complies with these requirements. The link count of directories which contain entries which are hard links to an ancestor will often be lower than they otherwise should be. This can mean that GNU find will sometimes optimise away the visiting of a subdirectory which is actually a link to an ancestor. Since find does not actually enter such a subdirectory, it is allowed to avoid emitting a diagnostic message. Although this behaviour may be somewhat confusing, it is unlikely that anybody actually depends on this behaviour. If the leaf optimisation has been turned off with -noleaf, the directory entry will always be examined and the diagnostic message will be issued where it is appropriate. Symbolic links cannot be used to create filesystem cycles as such, but if the -L option or the -follow option is in use, a diagnostic message is issued when find encounters a loop of symbolic links. As with loops containing hard links, the leaf optimisation will often mean that find knows that it doesn't need to call stat() or chdir() on the symbolic link, so this diagnostic is frequently not necessary.

The -d option is supported for compatibility with various BSD systems, but you should use the POSIX-compliant option -depth instead.

The POSIXLY_CORRECT environment variable does not affect the behaviour of the -regex or -iregex tests because those tests aren't specified in the POSIX standard.

Environment Variables

LANG

Provides a default value for the internationalization variables that are unset or null.

LC_ALL

If set to a non-empty string value, override the values of all the other internationalization variables.
LC_COLLATE
    The POSIX standard specifies that this variable affects the pattern matching to be used for the -name option. GNU find uses the fnmatch(3) library function, and so support for 'LC_COLLATE' depends on the system library. This variable also affects the interpretation of the response to -ok; while the 'LC_MESSAGES' variable selects the actual pattern used to interpret the response to -ok, the interpretation of any bracket expressions in the pattern will be affected by 'LC_COLLATE'. 
LC_CTYPE
    This variable affects the treatment of character classes used in regular expressions and also with the -name test, if the system's fnmatch(3) library function supports this. This variable also affects the interpretation of any character classes in the regular expressions used to interpret the response to the prompt issued by -ok. The 'LC_CTYPE' environment variable will also affect which characters are considered to be unprintable when filenames are printed; see the section UNUSUAL FILENAMES. 
LC_MESSAGES
    Determines the locale to be used for internationalised messages. If the 'POSIXLY_CORRECT' environment variable is set, this also determines the interpretation of the response to the prompt made by the -ok action. 
NLSPATH
    Determines the location of the internationalisation message catalogues. 
PATH

Affects the directories which are searched to find the executables invoked by -exec, -execdir, -ok and -okdir.
POSIXLY_CORRECT
    Determines the block size used by -ls and -fls. If POSIXLY_CORRECT is set, blocks are units of 512 bytes. Otherwise they are units of 1024 bytes. 
    Setting this variable also turns off warning messages (that is, implies -nowarn) by default, because POSIX requires that apart from the output for -ok, all messages printed on stderr are diagnostics and must result in a non-zero exit status.

    When POSIXLY_CORRECT is not set, -perm +zzz is treated just like -perm /zzz if +zzz is not a valid symbolic mode. When POSIXLY_CORRECT is set, such constructs are treated as an error.

    When POSIXLY_CORRECT is set, the response to the prompt made by the -ok action is interpreted according to the system's message catalogue, as opposed to according to find's own message translations. 
TZ

Affects the time zone used for some of the time-related format directives of -printf and -fprintf.

Examples

find /tmp -name core -type f -print | xargs /bin/rm -f

Find files named core in or below the directory /tmp and delete them. Note that this will work incorrectly if there are any filenames containing newlines, single or double quotes, or spaces.

find /tmp -name core -type f -print0 | xargs -0 /bin/rm -f

Find files named core in or below the directory /tmp and delete them, processing filenames in such a way that file or directory names containing single or double quotes, spaces or newlines are correctly handled. The -name test comes before the -type test in order to avoid having to call stat(2) on every file.

find . -type f -exec file '{}' \;

Runs 'file' on every file in or below the current directory. Notice that the braces are enclosed in single quote marks to protect them from interpretation as shell script punctuation. The semicolon is similarly protected by the use of a backslash, though single quotes could have been used in that case also.

find / \
\( -perm -4000 -fprintf /root/suid.txt %#m %u %p\n \) , \
\( -size +100M -fprintf /root/big.txt %-10s %p\n \)

Traverse the filesystem just once, listing setuid files and directories into /root/suid.txt and large files into /root/big.txt.

find $HOME -mtime 0

Search for files in your home directory which have been modified in the last twenty-four hours. This command works this way because the time since each file was last modified is divided by 24 hours and any remainder is discarded. That means that to match -mtime 0, a file will have to have a modification in the past which is less than 24 hours ago.

find /sbin /usr/sbin -executable \! -readable -print

Search for files which are executable but not readable.

find . -perm 664

Search for files which have read and write permission for their owner, and group, but which other users can read but not write to. Files which meet these criteria but have other permissions bits set (for example if someone can execute the file) will not be matched.

find . -perm -664

Search for files which have read and write permission for their owner and group, and which other users can read, without regard to the presence of any extra permission bits (for example the executable bit). This will match a file which has mode 0777, for example.

find . -perm /222

Search for files which are writable by somebody (their owner, or their group, or anybody else).

find . -perm /220
find . -perm /u+w,g+w
find . -perm /u=w,g=w

All three of these commands do the same thing, but the first one uses the octal representation of the file mode, and the other two use the symbolic form. These commands all search for files which are writable by either their owner or their group. The files don't have to be writable by both the owner and group to be matched; either will do.

find . -perm -220
find . -perm -g+w,u+w

Both these commands do the same thing; search for files which are writable by both their owner and their group.

find . -perm -444 -perm /222 ! -perm /111
find . -perm -a+r -perm /a+w ! -perm /a+x

These two commands both search for files that are readable for everybody ( -perm -444 or -perm -a+r), have at least one write bit set ( -perm /222 or -perm /a+w) but are not executable for anybody ( ! -perm /111 and ! -perm /a+x respectively).

cd /source-dir
find . -name .snapshot -prune -o \( \! -name *~ -print0 \)|
cpio -pmd0 /dest-dir

This command copies the contents of /source-dir to /dest-dir, but omits files and directories named .snapshot (and anything in them). It also omits files or directories whose name ends in ~, but not their contents. The construct -prune -o \( ... -print0 \) is quite common. The idea here is that the expression before -prune matches things which are to be pruned. However, the -prune action itself returns true, so the following -o ensures that the right hand side is evaluated only for those directories which didn't get pruned (the contents of the pruned directories are not even visited, so their contents are irrelevant). The expression on the right hand side of the -o is in parentheses only for clarity. It emphasises that the -print0 action takes place only for things that didn't have -prune applied to them. Because the default 'and' condition between tests binds more tightly than -o, this is the default anyway, but the parentheses help to show what is going on.

find repo/ -exec test -d {}/.svn -o -d {}/.git -o -d {}/CVS ; \
-print -prune

Given the following directory of projects and their associated SCM administrative directories, perform an efficient search for the projects' roots:

repo/project1/CVS
repo/gnu/project2/.svn
repo/gnu/project3/.svn
repo/gnu/project3/src/.svn
repo/project4/.git

In this example, -prune prevents unnecessary descent into directories that have already been discovered (for example we do not search project3/src because we already found project3/.svn), but ensures sibling directories (project2 and project3) are found.
Exit Status

find exits with status 0 if all files are processed successfully, greater than 0 if errors occur. This is deliberately a very broad description, but if the return value is non-zero, you should not rely on the correctness of the results of find. </p>

		<p class=quote>chmod</p>
		<p class=terminal>chmod(1) - Linux man page
Name
chmod - change file mode bits
Synopsis
chmod [OPTION]... MODE[,MODE]... FILE...
chmod [OPTION]... OCTAL-MODE FILE...
chmod [OPTION]... --reference=RFILE FILE...
Description
This manual page documents the GNU version of chmod. chmod changes the file mode bits of each given file according to mode, which can be either a symbolic representation of changes to make, or an octal number representing the bit pattern for the new mode bits.

The format of a symbolic mode is [ugoa...][[+-=][perms...]...], where perms is either zero or more letters from the set rwxXst, or a single letter from the set ugo. Multiple symbolic modes can be given, separated by commas.

A combination of the letters ugoa controls which users' access to the file will be changed: the user who owns it (u), other users in the file's group (g), other users not in the file's group (o), or all users (a). If none of these are given, the effect is as if a were given, but bits that are set in the umask are not affected.

The operator + causes the selected file mode bits to be added to the existing file mode bits of each file; - causes them to be removed; and = causes them to be added and causes unmentioned bits to be removed except that a directory's unmentioned set user and group ID bits are not affected.

The letters rwxXst select file mode bits for the affected users: read (r), write (w), execute (or search for directories) (x), execute/search only if the file is a directory or already has execute permission for some user (X), set user or group ID on execution (s), restricted deletion flag or sticky bit (t). Instead of one or more of these letters, you can specify exactly one of the letters ugo: the permissions granted to the user who owns the file (u), the permissions granted to other users who are members of the file's group (g), and the permissions granted to users that are in neither of the two preceding categories (o).

A numeric mode is from one to four octal digits (0-7), derived by adding up the bits with values 4, 2, and 1. Omitted digits are assumed to be leading zeros. The first digit selects the set user ID (4) and set group ID (2) and restricted deletion or sticky (1) attributes. The second digit selects permissions for the user who owns the file: read (4), write (2), and execute (1); the third selects permissions for other users in the file's group, with the same values; and the fourth for other users not in the file's group, with the same values.

chmod never changes the permissions of symbolic links; the chmod system call cannot change their permissions. This is not a problem since the permissions of symbolic links are never used. However, for each symbolic link listed on the command line, chmod changes the permissions of the pointed-to file. In contrast, chmod ignores symbolic links encountered during recursive directory traversals.
Setuid and Setgid Bits
chmod clears the set-group-ID bit of a regular file if the file's group ID does not match the user's effective group ID or one of the user's supplementary group IDs, unless the user has appropriate privileges. Additional restrictions may cause the set-user-ID and set-group-ID bits of MODE or RFILE to be ignored. This behavior depends on the policy and functionality of the underlying chmod system call. When in doubt, check the underlying system behavior.

chmod preserves a directory's set-user-ID and set-group-ID bits unless you explicitly specify otherwise. You can set or clear the bits with symbolic modes like u+s and g-s, and you can set (but not clear) the bits with a numeric mode.
Restricted Deletion Flag or Sticky Bit
The restricted deletion flag or sticky bit is a single bit, whose interpretation depends on the file type. For directories, it prevents unprivileged users from removing or renaming a file in the directory unless they own the file or the directory; this is called the restricted deletion flag for the directory, and is commonly found on world-writable directories like /tmp. For regular files on some older systems, the bit saves the program's text image on the swap device so it will load more quickly when run; this is called the sticky bit.
Options

Change the mode of each FILE to MODE.

-c, --changes
    like verbose but report only when a change is made 
--no-preserve-root
    do not treat '/' specially (the default) 
--preserve-root
    fail to operate recursively on '/' 
-f, --silent, --quiet
    suppress most error messages 
-v, --verbose
    output a diagnostic for every file processed 
--reference=RFILE
    use RFILE's mode instead of MODE values 
-R, --recursive
    change files and directories recursively 
--help
    display this help and exit 
--version
    output version information and exit

Each MODE is of the form '[ugoa]*([-+=]([rwxXst]*|[ugo]))+'.
Author
Written by David MacKenzie and Jim Meyering. </p>

		<p class=quote>df</p>
		<p class=terminal>df(1) - Linux man page
Name
df - report file system disk space usage
Synopsis
df [OPTION]... [FILE]...
Description
This manual page documents the GNU version of df. df displays the amount of disk space available on the file system containing each file name argument. If no file name is given, the space available on all currently mounted file systems is shown. Disk space is shown in 1K blocks by default, unless the environment variable POSIXLY_CORRECT is set, in which case 512-byte blocks are used.

If an argument is the absolute file name of a disk device node containing a mounted file system, df shows the space available on that file system rather than on the file system containing the device node (which is always the root file system). This version of df cannot show the space available on unmounted file systems, because on most kinds of systems doing so requires very nonportable intimate knowledge of file system structures.
Options

Show information about the file system on which each FILE resides, or all file systems by default.

Mandatory arguments to long options are mandatory for short options too.

-a, --all
    include dummy file systems 
-B, --block-size=SIZE
    use SIZE-byte blocks 
--direct
    show statistics for a file instead of mount point 
--total
    produce a grand total 
-h, --human-readable
    print sizes in human readable format (e.g., 1K 234M 2G) 
-H, --si
    likewise, but use powers of 1000 not 1024 
-i, --inodes
    list inode information instead of block usage 
-k
    like --block-size=1K 
-l, --local
    limit listing to local file systems 
--no-sync
    do not invoke sync before getting usage info (default) 
-P, --portability
    use the POSIX output format 
--sync
    invoke sync before getting usage info 
-t, --type=TYPE
    limit listing to file systems of type TYPE 
-T, --print-type
    print file system type 
-x, --exclude-type=TYPE
    limit listing to file systems not of type TYPE 
-v
    (ignored) 
--help
    display this help and exit 
--version
    output version information and exit

Display values are in units of the first available SIZE from --block-size, and the DF_BLOCK_SIZE, BLOCK_SIZE and BLOCKSIZE environment variables. Otherwise, units default to 1024 bytes (or 512 if POSIXLY_CORRECT is set).

SIZE may be (or may be an integer optionally followed by) one of following: KB 1000, K 1024, MB 1000*1000, M 1024*1024, and so on for G, T, P, E, Z, Y.
Author
Written by Torbjorn Granlund, David MacKenzie, and Paul Eggert. </p>

		<p class=quote>ip</p>
		<p class=terminal>ip(8) - Linux man page
Name

ip - show / manipulate routing, devices, policy routing and tunnels
Synopsis

ip [ OPTIONS ] OBJECT { COMMAND | help }

OBJECT := { link | addr | addrlabel | route | rule | neigh | tunnel | maddr | mroute | monitor }

OPTIONS := { -V[ersion] | -s[tatistics] | -r[esolve] | -f[amily] { inet | inet6 | ipx | dnet | link } | -o[neline] }

ip link set DEVICE { up | down | arp { on | off } |
promisc { on | off } |
allmulticast { on | off } |
dynamic { on | off } |
multicast { on | off } |
txqueuelen PACKETS |
name NEWNAME |
address LLADDR | broadcast LLADDR |
mtu MTU |
netns PID |
alias NAME |
vf NUM [ mac LLADDR ] [ vlan VLANID [ qos VLAN-QOS ] ] [ rate TXRATE ] }

ip link show [ DEVICE ]

ip addr { add | del } IFADDR dev STRING

ip addr { show | flush } [ dev STRING ] [ scope SCOPE-ID ] [ to PREFIX ] [ FLAG-LIST ] [ label PATTERN ]

IFADDR := PREFIX | ADDR peer PREFIX [ broadcast ADDR ] [ anycast ADDR ] [ label STRING ] [ scope SCOPE-ID ]

SCOPE-ID := [ host | link | global | NUMBER ]

FLAG-LIST := [ FLAG-LIST ] FLAG

FLAG := [ permanent | dynamic | secondary | primary | tentative | deprecated ]

ip addrlabel { add | del } prefix PREFIX [ dev DEV ] [ label NUMBER ]

ip addrlabel { list | flush }

ip route { list | flush } SELECTOR

ip route get ADDRESS [ from ADDRESS iif STRING ] [ oif STRING ] [ tos TOS ]

ip route { add | del | change | append | replace | monitor } ROUTE

SELECTOR := [ root PREFIX ] [ match PREFIX ] [ exact PREFIX ] [ table TABLE_ID ] [ proto RTPROTO ] [ type TYPE ] [ scope SCOPE ]

ROUTE := NODE_SPEC [ INFO_SPEC ]

NODE_SPEC := [ TYPE ] PREFIX [ tos TOS ] [ table TABLE_ID ] [ proto RTPROTO ] [ scope SCOPE ] [ metric METRIC ]

INFO_SPEC := NH OPTIONS FLAGS [ nexthop NH ] ...

NH := [ via ADDRESS ] [ dev STRING ] [ weight NUMBER ] NHFLAGS

OPTIONS := FLAGS [ mtu NUMBER ] [ advmss NUMBER ] [ rtt TIME ] [ rttvar TIME ] [ window NUMBER ] [ cwnd NUMBER ] [ initcwnd NUMBER ] [ ssthresh REALM ] [ realms REALM ] [ rto_min TIME ] [ initrwnd NUMBER ]

TYPE := [ unicast | local | broadcast | multicast | throw | unreachable | prohibit | blackhole | nat ]

TABLE_ID := [ local| main | default | all | NUMBER ]

SCOPE := [ host | link | global | NUMBER ]

FLAGS := [ equalize ]

NHFLAGS := [ onlink | pervasive ]

RTPROTO := [ kernel | boot | static | NUMBER ]

ip rule [ list | add | del | flush ] SELECTOR ACTION

SELECTOR := [ from PREFIX ] [ to PREFIX ] [ tos TOS ] [ fwmark FWMARK[/MASK] ] [ dev STRING ] [ pref NUMBER ]

ACTION := [ table TABLE_ID ] [ nat ADDRESS ] [ prohibit | reject | unreachable ] [ realms [SRCREALM/]DSTREALM ]

TABLE_ID := [ local | main | default | NUMBER ]

ip neigh { add | del | change | replace } { ADDR [ lladdr LLADDR ] [ nud { permanent | noarp | stale | reachable } ] | proxy ADDR } [ dev DEV ]

ip neigh { show | flush } [ to PREFIX ] [ dev DEV ] [ nud STATE ]

ip tunnel { add | change | del | show | prl } [ NAME ]
[ mode MODE ] [ remote ADDR ] [ local ADDR ]
[ [i|o]seq ] [ [i|o]key KEY ] [ [i|o]csum ] ]
[ encaplimit ELIM ] [ ttl TTL ]
[ tos TOS ] [ flowlabel FLOWLABEL ]
[ prl-default ADDR ] [ prl-nodefault ADDR ] [ prl-delete ADDR ]
[ [no]pmtudisc ] [ dev PHYS_DEV ] [ dscp inherit ]

MODE := { ipip | gre | sit | isatap | ip6ip6 | ipip6 | any }

ADDR := { IP_ADDRESS | any }

TOS := { NUMBER | inherit }

ELIM := { none | 0..255 }

TTL := { 1..255 | inherit }

KEY := { DOTTED_QUAD | NUMBER }

TIME := NUMBER[s|ms|us|ns|j]

ip maddr [ add | del ] MULTIADDR dev STRING

ip maddr show [ dev STRING ]

ip mroute show [ PREFIX ] [ from PREFIX ] [ iif DEVICE ]

ip monitor [ all | LISTofOBJECTS ]

ip xfrm XFRM_OBJECT { COMMAND }

XFRM_OBJECT := { state | policy | monitor }

ip xfrm state { add | update } ID [ XFRM_OPT ] [ mode MODE ]
[ reqid REQID ] [ seq SEQ ] [ replay-window SIZE ]
[ flag FLAG-LIST ] [ encap ENCAP ] [ sel SELECTOR ]
[ LIMIT-LIST ]

ip xfrm state allocspi ID [ mode MODE ] [ reqid REQID ] [ seq SEQ ] [ min SPI max SPI ]

ip xfrm state { delete | get } ID

ip xfrm state { deleteall | list } [ ID ] [ mode MODE ]
[ reqid REQID ] [ flag FLAG_LIST ]

ip xfrm state flush [ proto XFRM_PROTO ]

ip xfrm state count

ID := [ src ADDR ] [ dst ADDR ] [ proto XFRM_PROTO ] [ spi SPI ]

XFRM_PROTO := [ esp | ah | comp | route2 | hao ]

MODE := [ transport | tunnel | ro | beet ] (default=transport)

FLAG-LIST := [ FLAG-LIST ] FLAG

FLAG := [ noecn | decap-dscp | wildrecv ]

ENCAP := ENCAP-TYPE SPORT DPORT OADDR

ENCAP-TYPE := espinudp | espinudp-nonike

ALGO-LIST := [ ALGO-LIST ] | [ ALGO ]

ALGO := ALGO_TYPE ALGO_NAME ALGO_KEY

ALGO_TYPE := [ enc | auth | comp ]

SELECTOR := src ADDR[/PLEN] dst ADDR[/PLEN] [ UPSPEC ] [ dev DEV ]

UPSPEC := proto PROTO [[ sport PORT ] [ dport PORT ] |
[ type NUMBER ] [ code NUMBER ]]

LIMIT-LIST := [ LIMIT-LIST ] | [ limit LIMIT ]

LIMIT := [ [time-soft|time-hard|time-use-soft|time-use-hard] SECONDS ] | [ [byte-soft|byte-hard] SIZE ] |
[ [packet-soft|packet-hard] COUNT ]

ip xfrm policy { add | update } dir DIR SELECTOR [ index INDEX ]
[ ptype PTYPE ] [ action ACTION ] [ priority PRIORITY ]
[ LIMIT-LIST ] [ TMPL-LIST ]

ip xfrm policy { delete | get } dir DIR [ SELECTOR | index INDEX ]
[ ptype PTYPE ]

ip xfrm policy { deleteall | list } [ dir DIR ] [ SELECTOR ]
[ index INDEX ] [ action ACTION ] [ priority PRIORITY ]

ip xfrm policy flush [ ptype PTYPE ]

ip xfrm count

PTYPE := [ main | sub ] (default=main)

DIR := [ in | out | fwd ]

SELECTOR := src ADDR[/PLEN] dst ADDR[/PLEN] [ UPSPEC ] [ dev DEV ]

UPSPEC := proto PROTO [ [ sport PORT ] [ dport PORT ] |
[ type NUMBER ] [ code NUMBER ] ]

ACTION := [ allow | block ] (default=allow)

LIMIT-LIST := [ LIMIT-LIST ] | [ limit LIMIT ]

LIMIT := [ [time-soft|time-hard|time-use-soft|time-use-hard] SECONDS ] | [ [byte-soft|byte-hard] SIZE ] |
[packet-soft|packet-hard] NUMBER ]

TMPL-LIST := [ TMPL-LIST ] | [ tmpl TMPL ]

TMPL := ID [ mode MODE ] [ reqid REQID ] [ level LEVEL ]

ID := [ src ADDR ] [ dst ADDR ] [ proto XFRM_PROTO ] [ spi SPI ]

XFRM_PROTO := [ esp | ah | comp | route2 | hao ]

MODE := [ transport | tunnel | beet ] (default=transport)

LEVEL := [ required | use ] (default=required)

ip xfrm monitor [ all | LISTofOBJECTS ]
Options

-V, -Version

    print the version of the ip utility and exit. 
-s, -stats, -statistics
    output more information. If the option appears twice or more, the amount of information increases. As a rule, the information is statistics or some time values. 
-f, -family
    followed by protocol family identifier: inet, inet6 or link ,enforce the protocol family to use. If the option is not present, the protocol family is guessed from other arguments. If the rest of the command line does not give enough information to guess the family, ip falls back to the default one, usually inet or any. link is a special family identifier meaning that no networking protocol is involved. 
-4

shortcut for -family inet.

-6

shortcut for -family inet6.

-0

shortcut for -family link.
-o, -oneline
    output each record on a single line, replacing line feeds with the '\' character. This is convenient when you want to count records with wc(1) or to grep(1) the output. 
-r, -resolve
    use the system's name resolver to print DNS names instead of host addresses. 

Ip - Command Syntax

OBJECT

link

- network device.
address
    - protocol (IP or IPv6) address on a device. 
addrlabel
    - label configuration for protocol address selection. 
neighbour
    - ARP or NDISC cache entry. 
route

- routing table entry.

rule

- rule in routing policy database.
maddress
    - multicast address. 
mroute

- multicast routing cache entry.

tunnel

- tunnel over IP.

xfrm

- framework for IPsec protocol.
The names of all objects may be written in full or abbreviated form, f.e. address is abbreviated as addr or just a.

COMMAND

Specifies the action to perform on the object. The set of possible actions depends on the object type. As a rule, it is possible to add, delete and show (or list ) objects, but some objects do not allow all of these operations or have some additional commands. The help command is available for all objects. It prints out a list of available commands and argument syntax conventions.

If no command is given, some default command is assumed. Usually it is list or, if the objects of this class cannot be listed, help.

ip link - network device configuration

link is a network device and the corresponding commands display and change the state of devices.

ip link set - change device attributes

dev NAME (default)
    NAME specifies network device to operate on. When configuring SR-IOV Virtual Fuction (VF) devices, this keyword should specify the associated Physical Function (PF) device. 
up and down
    change the state of the device to UP or DOWN. 
arp on or arp off
    change the NOARP flag on the device. 
multicast on or multicast off
    change the MULTICAST flag on the device. 
dynamic on or dynamic off
    change the DYNAMIC flag on the device. 
name NAME
    change the name of the device. This operation is not recommended if the device is running or has some addresses already configured. 
txqueuelen NUMBER
txqlen NUMBER
    change the transmit queue length of the device. 
mtu NUMBER
    change the MTU of the device. 
address LLADDRESS
    change the station address of the interface. 
broadcast LLADDRESS
brd LLADDRESS
peer LLADDRESS
    change the link layer broadcast address or the peer address when the interface is POINTOPOINT. 
netns PID
    move the device to the network namespace associated with the process PID. 
alias NAME
    give the device a symbolic name for easy reference. 
vf NUM

specify a Virtual Function device to be configured. The associated PF device must be specified using the dev parameter.

mac LLADDRESS - change the station address for the specified VF. The vf parameter must be specified.

vlan VLANID - change the assigned VLAN for the specified VF. When specified, all traffic sent from the VF will be tagged with the specified VLAN ID. Incoming traffic will be filtered for the specified VLAN ID, and will have all VLAN tags stripped before being passed to the VF. Setting this parameter to 0 disables VLAN tagging and filtering. The vf parameter must be specified.

qos VLAN-QOS - assign VLAN QOS (priority) bits for the VLAN tag. When specified, all VLAN tags transmitted by the VF will include the specified priority bits in the VLAN tag. If not specified, the value is assumed to be 0. Both the vf and vlan parameters must be specified. Setting both vlan and qos as 0 disables VLAN tagging and filtering for the VF.

rate TXRATE - change the allowed transmit bandwidth, in Mbps, for the specified VF. Setting this parameter to 0 disables rate limiting. The vf parameter must be specified.

Warning: If multiple parameter changes are requested, ip aborts immediately after any of the changes have failed. This is the only case when ip can move the system to an unpredictable state. The solution is to avoid changing several parameters with one ip link set call.

ip link show - display device attributes

dev NAME (default)
    NAME specifies the network device to show. If this argument is omitted all devices are listed. 
up

only display running interfaces.

ip address - protocol address management.

The address is a protocol (IP or IPv6) address attached to a network device. Each device must have at least one address to use the corresponding protocol. It is possible to have several different addresses attached to one device. These addresses are not discriminated, so that the term alias is not quite appropriate for them and we do not use it in this document.

The ip addr command displays addresses and their properties, adds new addresses and deletes old ones.

ip address add - add new protocol address.

dev NAME
    the name of the device to add the address to. 
local ADDRESS (default)
    the address of the interface. The format of the address depends on the protocol. It is a dotted quad for IP and a sequence of hexadecimal halfwords separated by colons for IPv6. The ADDRESS may be followed by a slash and a decimal number which encodes the network prefix length. 
peer ADDRESS
    the address of the remote endpoint for pointopoint interfaces. Again, the ADDRESS may be followed by a slash and a decimal number, encoding the network prefix length. If a peer address is specified, the local address cannot have a prefix length. The network prefix is associated with the peer rather than with the local address. 
broadcast ADDRESS
    the broadcast address on the interface.

    It is possible to use the special symbols '+' and '-' instead of the broadcast address. In this case, the broadcast address is derived by setting/resetting the host bits of the interface prefix. 
label NAME
    Each address may be tagged with a label string. In order to preserve compatibility with Linux-2.0 net aliases, this string must coincide with the name of the device or must be prefixed with the device name followed by colon. 
scope SCOPE_VALUE
    the scope of the area where this address is valid. The available scopes are listed in file /etc/iproute2/rt_scopes. Predefined scope values are: 

global - the address is globally valid.

site - (IPv6 only) the address is site local, i.e. it is valid inside this site.

link - the address is link local, i.e. it is valid only on this device.

host - the address is valid only inside this host.

ip address delete - delete protocol address

Arguments: coincide with the arguments of ip addr add. The device name is a required argument. The rest are optional. If no arguments are given, the first address is deleted.

ip address show - look at protocol addresses

dev NAME (default)
    name of device. 
scope SCOPE_VAL
    only list addresses with this scope. 
to PREFIX
    only list addresses matching this prefix. 
label PATTERN
    only list addresses with labels matching the PATTERN. PATTERN is a usual shell style pattern. 
dynamic and permanent
    (IPv6 only) only list addresses installed due to stateless address configuration or only list permanent (not dynamic) addresses. 
tentative
    (IPv6 only) only list addresses which did not pass duplicate address detection. 
deprecated
    (IPv6 only) only list deprecated addresses. 
primary and secondary
    only list primary (or secondary) addresses. 

ip address flush - flush protocol addresses

This command flushes the protocol addresses selected by some criteria.

This command has the same arguments as show. The difference is that it does not run when no arguments are given.

Warning: This command (and other flush commands described below) is pretty dangerous. If you make a mistake, it will not forgive it, but will cruelly purge all the addresses.

With the -statistics option, the command becomes verbose. It prints out the number of deleted addresses and the number of rounds made to flush the address list. If this option is given twice, ip addr flush also dumps all the deleted addresses in the format described in the previous subsection.

ip addrlabel - protocol address label management.

IPv6 address label is used for address selection described in RFC 3484. Precedence is managed by userspace, and only label is stored in kernel.

ip addrlabel add - add an address label

the command adds an address label entry to the kernel.
prefix PREFIX
dev DEV
    the outgoing interface. 
label NUMBER
    the label for the prefix. 0xffffffff is reserved. 

ip addrlabel del - delete an address label

the command deletes an address label entry in the kernel. Arguments: coincide with the arguments of ip addrlabel add but label is not required.

ip addrlabel list - list address labels

the command show contents of address labels.

ip addrlabel flush - flush address labels

the command flushes the contents of address labels and it does not restore default settings.

ip neighbour - neighbour/arp tables management.

neighbour objects establish bindings between protocol addresses and link layer addresses for hosts sharing the same link. Neighbour entries are organized into tables. The IPv4 neighbour table is known by another name - the ARP table.

The corresponding commands display neighbour bindings and their properties, add new neighbour entries and delete old ones.

ip neighbour add - add a new neighbour entry

ip neighbour change - change an existing entry

ip neighbour replace - add a new entry or change an existing one

These commands create new neighbour records or update existing ones.
to ADDRESS (default)
    the protocol address of the neighbour. It is either an IPv4 or IPv6 address. 
dev NAME
    the interface to which this neighbour is attached. 
lladdr LLADDRESS
    the link layer address of the neighbour. LLADDRESS can also be null. 
nud NUD_STATE
    the state of the neighbour entry. nud is an abbreviation for 'Neigh bour Unreachability Detection'. The state can take one of the following values: 

permanent - the neighbour entry is valid forever and can be only be removed administratively.

noarp - the neighbour entry is valid. No attempts to validate this entry will be made but it can be removed when its lifetime expires.

reachable - the neighbour entry is valid until the reachability timeout expires.

stale - the neighbour entry is valid but suspicious. This option to ip neigh does not change the neighbour state if it was valid and the address is not changed by this command.

ip neighbour delete - delete a neighbour entry

This command invalidates a neighbour entry.

The arguments are the same as with ip neigh add, except that lladdr and nud are ignored.

Warning: Attempts to delete or manually change a noarp entry created by the kernel may result in unpredictable behaviour. Particularly, the kernel may try to resolve this address even on a NOARP interface or if the address is multicast or broadcast.

ip neighbour show - list neighbour entries

This commands displays neighbour tables.
to ADDRESS (default)
    the prefix selecting the neighbours to list. 
dev NAME
    only list the neighbours attached to this device. 
unused

only list neighbours which are not currently in use.
nud NUD_STATE
    only list neighbour entries in this state. NUD_STATE takes values listed below or the special value all which means all states. This option may occur more than once. If this option is absent, ip lists all entries except for none and noarp. 

ip neighbour flush - flush neighbour entries

This command flushes neighbour tables, selecting entries to flush by some criteria.

This command has the same arguments as show. The differences are that it does not run when no arguments are given, and that the default neighbour states to be flushed do not include permanent and noarp.

With the -statistics option, the command becomes verbose. It prints out the number of deleted neighbours and the number of rounds made to flush the neighbour table. If the option is given twice, ip neigh flush also dumps all the deleted neighbours.

ip route - routing table management

Manipulate route entries in the kernel routing tables keep information about paths to other networked nodes.

Route types:

    unicast - the route entry describes real paths to the destinations covered by the route prefix.

    unreachable - these destinations are unreachable. Packets are discarded and the ICMP message host unreachable is generated. The local senders get an EHOSTUNREACH error.

    blackhole - these destinations are unreachable. Packets are discarded silently. The local senders get an EINVAL error.

    prohibit - these destinations are unreachable. Packets are discarded and the ICMP message communication administratively prohibited is generated. The local senders get an EACCES error.

    local - the destinations are assigned to this host. The packets are looped back and delivered locally.

    broadcast - the destinations are broadcast addresses. The packets are sent as link broadcasts.

    throw - a special control route used together with policy rules. If such a route is selected, lookup in this table is terminated pretending that no route was found. Without policy routing it is equivalent to the absence of the route in the routing table. The packets are dropped and the ICMP message net unreachable is generated. The local senders get an ENETUNREACH error.

    nat - a special NAT route. Destinations covered by the prefix are considered to be dummy (or external) addresses which require translation to real (or internal) ones before forwarding. The addresses to translate to are selected with the attribute Warning: Route NAT is no longer supported in Linux 2.6.

    via.

    anycast - not implemented the destinations are anycast addresses assigned to this host. They are mainly equivalent to local with one difference: such addresses are invalid when used as the source address of any packet.

    multicast - a special type used for multicast routing. It is not present in normal routing tables. 
Route tables: Linux-2.x can pack routes into several routing tables identified by a number in the range from 1 to 255 or by name from the file /etc/iproute2/rt_tables By default all normal routes are inserted into the main table (ID 254) and the kernel only uses this table when calculating routes.

Actually, one other table always exists, which is invisible but even more important. It is the local table (ID 255). This table consists of routes for local and broadcast addresses. The kernel maintains this table automatically and the administrator usually need not modify it or even look at it.

The multiple routing tables enter the game when policy routing is used.

ip route add - add new route

ip route change - change route

ip route replace - change or add new one

to TYPE PREFIX (default)
    the destination prefix of the route. If TYPE is omitted, ip assumes type unicast. Other values of TYPE are listed above. PREFIX is an IP or IPv6 address optionally followed by a slash and the prefix length. If the length of the prefix is missing, ip assumes a full-length host route. There is also a special PREFIX default - which is equivalent to IP 0/0 or to IPv6 ::/0. 
tos TOS
dsfield TOS
    the Type Of Service (TOS) key. This key has no associated mask and the longest match is understood as: First, compare the TOS of the route and of the packet. If they are not equal, then the packet may still match a route with a zero TOS. TOS is either an 8 bit hexadecimal number or an identifier from /etc/iproute2/rt_dsfield. 
metric NUMBER
preference NUMBER
    the preference value of the route. NUMBER is an arbitrary 32bit number. 
table TABLEID
    the table to add this route to. TABLEID may be a number or a string from the file /etc/iproute2/rt_tables. If this parameter is omitted, ip assumes the main table, with the exception of local , broadcast and nat routes, which are put into the local table by default. 
dev NAME
    the output device name. 
via ADDRESS
    the address of the nexthop router. Actually, the sense of this field depends on the route type. For normal unicast routes it is either the true next hop router or, if it is a direct route installed in BSD compatibility mode, it can be a local address of the interface. For NAT routes it is the first address of the block of translated IP destinations. 
src ADDRESS
    the source address to prefer when sending to the destinations covered by the route prefix. 
realm REALMID
    the realm to which this route is assigned. REALMID may be a number or a string from the file /etc/iproute2/rt_realms. 
mtu MTU
mtu lock MTU
    the MTU along the path to the destination. If the modifier lock is not used, the MTU may be updated by the kernel due to Path MTU Discovery. If the modifier lock is used, no path MTU discovery will be tried, all packets will be sent without the DF bit in IPv4 case or fragmented to MTU for IPv6. 
window NUMBER
    the maximal window for TCP to advertise to these destinations, measured in bytes. It limits maximal data bursts that our TCP peers are allowed to send to us. 
rtt TIME
    the initial RTT ('Round Trip Time') estimate. If no suffix is specified the units are raw values passed directly to the routing code to maintain compatability with previous releases. Otherwise if a suffix of s, sec or secs is used to specify seconds; ms, msec or msecs to specify milliseconds; us, usec or usecs to specify microseconds; ns, nsec or nsecs to specify nanoseconds; j, hz or jiffies to specify jiffies, the value is converted to what the routing code expects. 
rttvar TIME (2.3.15+ only)
    the initial RTT variance estimate. Values are specified as with rtt above. 
rto_min TIME (2.6.23+ only)
    the minimum TCP Retransmission TimeOut to use when communicating with this destination. Values are specified as with rtt above. 
ssthresh NUMBER (2.3.15+ only)
    an estimate for the initial slow start threshold. 
cwnd NUMBER (2.3.15+ only)
    the clamp for congestion window. It is ignored if the lock flag is not used. 
initcwnd NUMBER
    the maximum initial congestion window (cwnd) size in MSS of a TCP connection. 
initrwnd NUMBER (2.6.33+ only)
    the initial receive window size for connections to this destination. Actual window size is this value multiplied by the MSS of the connection. The default value is zero, meaning to use Slow Start value. 
advmss NUMBER (2.3.15+ only)
    the MSS ('Maximal Segment Size') to advertise to these destinations when establishing TCP connections. If it is not given, Linux uses a default value calculated from the first hop device MTU. (If the path to these destination is asymmetric, this guess may be wrong.) 
reordering NUMBER (2.3.15+ only)
    Maximal reordering on the path to this destination. If it is not given, Linux uses the value selected with sysctl variable net/ipv4/tcp_reordering. 
nexthop NEXTHOP
    the nexthop of a multipath route. NEXTHOP is a complex value with its own syntax similar to the top level argument lists: 

via ADDRESS - is the nexthop router.

dev NAME - is the output device.

weight NUMBER - is a weight for this element of a multipath route reflecting its relative bandwidth or quality.

scope SCOPE_VAL
    the scope of the destinations covered by the route prefix. SCOPE_VAL may be a number or a string from the file /etc/iproute2/rt_scopes. If this parameter is omitted, ip assumes scope global for all gatewayed unicast routes, scope link for direct unicast and broadcast routes and scope host for local routes. 
protocol RTPROTO
    the routing protocol identifier of this route. RTPROTO may be a number or a string from the file /etc/iproute2/rt_protos. If the routing protocol ID is not given, ip assumes protocol boot (i.e. it assumes the route was added by someone who doesn't understand what they are doing). Several protocol values have a fixed interpretation. Namely: 

redirect - the route was installed due to an ICMP redirect.

kernel - the route was installed by the kernel during autoconfiguration.

boot - the route was installed during the bootup sequence. If a routing daemon starts, it will purge all of them.

static - the route was installed by the administrator to override dynamic routing. Routing daemon will respect them and, probably, even advertise them to its peers.

ra - the route was installed by Router Discovery protocol.

    The rest of the values are not reserved and the administrator is free to assign (or not to assign) protocol tags. 
onlink

pretend that the nexthop is directly attached to this link, even if it does not match any interface prefix.
equalize
    allow packet by packet randomization on multipath routes. Without this modifier, the route will be frozen to one selected nexthop, so that load splitting will only occur on per-flow base. equalize only works if the kernel is patched. 

ip route delete - delete route

ip route del has the same arguments as ip route add, but their semantics are a bit different.

Key values (to, tos, preference and table) select the route to delete. If optional attributes are present, ip verifies that they coincide with the attributes of the route to delete. If no route with the given key and attributes was found, ip route del fails.

ip route show - list routes

the command displays the contents of the routing tables or the route(s) selected by some criteria.
to SELECTOR (default)
    only select routes from the given range of destinations. SELECTOR consists of an optional modifier (root, match or exact) and a prefix. root PREFIX selects routes with prefixes not shorter than PREFIX. F.e. root 0/0 selects the entire routing table. match PREFIX selects routes with prefixes not longer than PREFIX. F.e. match 10.0/16 selects 10.0/16, 10/8 and 0/0, but it does not select 10.1/16 and 10.0.0/24. And exact PREFIX (or just PREFIX) selects routes with this exact prefix. If neither of these options are present, ip assumes root 0/0 i.e. it lists the entire table. 
tos TOS
    dsfield TOS only select routes with the given TOS. 
table TABLEID
    show the routes from this table(s). The default setting is to show tablemain. TABLEID may either be the ID of a real table or one of the special values: 

all - list all of the tables.

cache - dump the routing cache.

cloned

cached

list cloned routes i.e. routes which were dynamically forked from other routes because some route attribute (f.e. MTU) was updated. Actually, it is equivalent to table cache.
from SELECTOR
    the same syntax as for to, but it binds the source address range rather than destinations. Note that the from option only works with cloned routes. 
protocol RTPROTO
    only list routes of this protocol. 
scope SCOPE_VAL
    only list routes with this scope. 
type TYPE
    only list routes of this type. 
dev NAME
    only list routes going via this device. 
via PREFIX
    only list routes going via the nexthop routers selected by PREFIX. 
src PREFIX
    only list routes with preferred source addresses selected by PREFIX. 
realm REALMID
realms FROMREALM/TOREALM
    only list routes with these realms. 

ip route flush - flush routing tables

this command flushes routes selected by some criteria.

The arguments have the same syntax and semantics as the arguments of ip route show, but routing tables are not listed but purged. The only difference is the default action: show dumps all the IP main routing table but flush prints the helper page.

With the -statistics option, the command becomes verbose. It prints out the number of deleted routes and the number of rounds made to flush the routing table. If the option is given twice, ip route flush also dumps all the deleted routes in the format described in the previous subsection.

ip route get - get a single route

this command gets a single route to a destination and prints its contents exactly as the kernel sees it.
to ADDRESS (default)
    the destination address. 
from ADDRESS
    the source address. 
tos TOS
dsfield TOS
    the Type Of Service. 
iif NAME
    the device from which this packet is expected to arrive. 
oif NAME
    force the output device on which this packet will be routed. 
connected
    if no source address (option from) was given, relookup the route with the source set to the preferred address received from the first lookup. If policy routing is used, it may be a different route. 
Note that this operation is not equivalent to ip route show. show shows existing routes. get resolves them and creates new clones if necessary. Essentially, get is equivalent to sending a packet along this path. If the iif argument is not given, the kernel creates a route to output packets towards the requested destination. This is equivalent to pinging the destination with a subsequent ip route ls cache, however, no packets are actually sent. With the iif argument, the kernel pretends that a packet arrived from this interface and searches for a path to forward the packet.

ip rule - routing policy database management

Rules in the routing policy database control the route selection algorithm.

Classic routing algorithms used in the Internet make routing decisions based only on the destination address of packets (and in theory, but not in practice, on the TOS field).

In some circumstances we want to route packets differently depending not only on destination addresses, but also on other packet fields: source address, IP protocol, transport protocol ports or even packet payload. This task is called 'policy routing'.

To solve this task, the conventional destination based routing table, ordered according to the longest match rule, is replaced with a 'routing policy database' (or RPDB), which selects routes by executing some set of rules.

Each policy routing rule consists of a selector and an action predicate. The RPDB is scanned in the order of increasing priority. The selector of each rule is applied to {source address, destination address, incoming interface, tos, fwmark} and, if the selector matches the packet, the action is performed. The action predicate may return with success. In this case, it will either give a route or failure indication and the RPDB lookup is terminated. Otherwise, the RPDB program continues on the next rule.

Semantically, natural action is to select the nexthop and the output device.

At startup time the kernel configures the default RPDB consisting of three rules:

1.

Priority: 0, Selector: match anything, Action: lookup routing table local (ID 255). The local table is a special routing table containing high priority control routes for local and broadcast addresses.
Rule 0 is special. It cannot be deleted or overridden.
2.

Priority: 32766, Selector: match anything, Action: lookup routing table main (ID 254). The main table is the normal routing table containing all non-policy routes. This rule may be deleted and/or overridden with other ones by the administrator.

3.

Priority: 32767, Selector: match anything, Action: lookup routing table default (ID 253). The default table is empty. It is reserved for some post-processing if no previous default rules selected the packet. This rule may also be deleted.
Each RPDB entry has additional attributes. F.e. each rule has a pointer to some routing table. NAT and masquerading rules have an attribute to select new IP address to translate/masquerade. Besides that, rules have some optional attributes, which routes have, namely realms. These values do not override those contained in the routing tables. They are only used if the route did not select any attributes.

The RPDB may contain rules of the following types:
    unicast - the rule prescribes to return the route found in the routing table referenced by the rule.

    blackhole - the rule prescribes to silently drop the packet.

    unreachable - the rule prescribes to generate a 'Network is unreachable' error.

    prohibit - the rule prescribes to generate 'Communication is administratively prohibited' error.

    nat - the rule prescribes to translate the source address of the IP packet into some other value. 

ip rule add - insert a new rule

ip rule delete - delete a rule

type TYPE (default)
    the type of this rule. The list of valid types was given in the previous subsection. 
from PREFIX
    select the source prefix to match. 
to PREFIX
    select the destination prefix to match. 
iif NAME
    select the incoming device to match. If the interface is loopback, the rule only matches packets originating from this host. This means that you may create separate routing tables for forwarded and local packets and, hence, completely segregate them. 
tos TOS
dsfield TOS
    select the TOS value to match. 
fwmark MARK
    select the fwmark value to match. 
priority PREFERENCE
    the priority of this rule. Each rule should have an explicitly set unique priority value. The options preference and order are synonyms with priority. 
table TABLEID
    the routing table identifier to lookup if the rule selector matches. It is also possible to use lookup instead of table. 
realms FROM/TO
    Realms to select if the rule matched and the routing table lookup succeeded. Realm TO is only used if the route did not select any realm. 
nat ADDRESS
    The base of the IP address block to translate (for source addresses). The ADDRESS may be either the start of the block of NAT addresses (selected by NAT routes) or a local host address (or even zero). In the last case the router does not translate the packets, but masquerades them to this address. Using map-to instead of nat means the same thing.

    Warning: Changes to the RPDB made with these commands do not become active immediately. It is assumed that after a script finishes a batch of updates, it flushes the routing cache with ip route flush cache. 

ip rule flush - also dumps all the deleted rules.

This command has no arguments.

ip rule show - list rules

This command has no arguments. The options list or lst are synonyms with show.

ip maddress - multicast addresses management

maddress objects are multicast addresses.

ip maddress show - list multicast addresses

dev NAME (default)
    the device name. 

ip maddress add - add a multicast address

ip maddress delete - delete a multicast address

these commands attach/detach a static link layer multicast address to listen on the interface. Note that it is impossible to join protocol multicast groups statically. This command only manages link layer addresses.
address LLADDRESS (default)
    the link layer multicast address. 
dev NAME
    the device to join/leave this multicast address. 

ip mroute - multicast routing cache management

mroute objects are multicast routing cache entries created by a user level mrouting daemon (f.e. pimd or mrouted ).

Due to the limitations of the current interface to the multicast routing engine, it is impossible to change mroute objects administratively, so we may only display them. This limitation will be removed in the future.

ip mroute show - list mroute cache entries

to PREFIX (default)
    the prefix selecting the destination multicast addresses to list. 
iif NAME
    the interface on which multicast packets are received. 
from PREFIX
    the prefix selecting the IP source addresses of the multicast route. 

ip tunnel - tunnel configuration

tunnel objects are tunnels, encapsulating packets in IP packets and then sending them over the IP infrastructure. The encapulating (or outer) address family is specified by the -f option. The default is IPv4.

ip tunnel add - add a new tunnel

ip tunnel change - change an existing tunnel

ip tunnel delete - destroy a tunnel

name NAME (default)
    select the tunnel device name. 
mode MODE
    set the tunnel mode. Available modes depend on the encapsulating address family.
    Modes for IPv4 encapsulation available: ipip, sit, isatap and gre.
    Modes for IPv6 encapsulation available: ip6ip6, ipip6 and any. 
remote ADDRESS
    set the remote endpoint of the tunnel. 
local ADDRESS
    set the fixed local address for tunneled packets. It must be an address on another interface of this host. 
ttl N

set a fixed TTL N on tunneled packets. N is a number in the range 1--255. 0 is a special value meaning that packets inherit the TTL value. The default value for IPv4 tunnels is: inherit. The default value for IPv6 tunnels is: 64.

tos T
dsfield T
tclass T
    set a fixed TOS (or traffic class in IPv6) T on tunneled packets. The default value is: inherit. 
dev NAME
    bind the tunnel to the device NAME so that tunneled packets will only be routed via this device and will not be able to escape to another device when the route to endpoint changes. 
nopmtudisc
    disable Path MTU Discovery on this tunnel. It is enabled by default. Note that a fixed ttl is incompatible with this option: tunnelling with a fixed ttl always makes pmtu discovery. 
key K

ikey K

okey K

( only GRE tunnels ) use keyed GRE with key K. K is either a number or an IP address-like dotted quad. The key parameter sets the key to use in both directions. The ikey and okey parameters set different keys for input and output.
csum, icsum, ocsum
    ( only GRE tunnels ) generate/require checksums for tunneled packets. The ocsum flag calculates checksums for outgoing packets. The icsum flag requires that all input packets have the correct checksum. The csum flag is equivalent to the combination icsum ocsum. 
seq, iseq, oseq
    ( only GRE tunnels ) serialize packets. The oseq flag enables sequencing of outgoing packets. The iseq flag requires that all input packets are serialized. The seq flag is equivalent to the combination iseq oseq. It isn't work. Don't use it. 
dscp inherit
    ( only IPv6 tunnels ) Inherit DS field between inner and outer header. 
encaplim ELIM
    ( only IPv6 tunnels ) set a fixed encapsulation limit. Default is 4. 
flowlabel FLOWLABEL
    ( only IPv6 tunnels ) set a fixed flowlabel. 

ip tunnel prl - potential router list (ISATAP only)

dev NAME
    mandatory device name. 
prl-default ADDR
prl-nodefault ADDR
prl-delete ADDR
    Add or delete ADDR as a potential router or default router. 

ip tunnel show - list tunnels

This command has no arguments.

ip monitor and rtmon - state monitoring

The ip utility can monitor the state of devices, addresses and routes continuously. This option has a slightly different format. Namely, the monitor command is the first in the command line and then the object list follows:

ip monitor [ all | LISTofOBJECTS ]

OBJECT-LIST is the list of object types that we want to monitor. It may contain link, address and route. If no file argument is given, ip opens RTNETLINK, listens on it and dumps state changes in the format described in previous sections.

If a file name is given, it does not listen on RTNETLINK, but opens the file containing RTNETLINK messages saved in binary format and dumps them. Such a history file can be generated with the rtmon utility. This utility has a command line syntax similar to ip monitor. Ideally, rtmon should be started before the first network configuration command is issued. F.e. if you insert:

    rtmon file /var/log/rtmon.log 
in a startup script, you will be able to view the full history later.

Certainly, it is possible to start rtmon at any time. It prepends the history with the state snapshot dumped at the moment of starting.

ip xfrm - setting xfrm

xfrm is an IP framework, which can transform format of the datagrams,
i.e. encrypt the packets with some algorithm. xfrm policy and xfrm state are associated through templates TMPL_LIST. This framework is used as a part of IPsec protocol.

ip xfrm state add - add new state into xfrm

ip xfrm state update - update existing xfrm state

ip xfrm state allocspi - allocate SPI value

MODE

is set as default to transport, but it could be set to tunnel,ro or beet.
FLAG-LIST
    contains one or more flags. 
FLAG

could be set to noecn, decap-dscp or wildrecv.

ENCAP

encapsulation is set to encapsulation type ENCAP-TYPE, source port SPORT, destination port DPORT and OADDR.
ENCAP-TYPE
    could be set to espinudp or espinudp-nonike. 
ALGO-LIST
    contains one or more algorithms ALGO which depend on the type of algorithm set by ALGO_TYPE. It can be used these algoritms enc, auth or comp. 

ip xfrm policy add - add a new policy

ip xfrm policy update - update an existing policy

ip xfrm policy delete - delete existing policy

ip xfrm policy get - get existing policy

ip xfrm policy deleteall - delete all existing xfrm policy

ip xfrm policy list - print out the list of xfrm policy

ip xfrm policy flush - flush policies

It can be flush all policies or only those specified with ptype.
dir DIR
    directory could be one of these: inp, out or fwd. 
SELECTOR
    selects for which addresses will be set up the policy. The selector is defined by source and destination address. 
UPSPEC

is defined by source port sport, destination port dport, type as number and code also number.
dev DEV
    specify network device. 
index INDEX
    the number of indexed policy. 
ptype PTYPE
    type is set as default on main, could be switch on sub. 
action ACTION
    is set as default on allow. It could be switch on block. 
priority PRIORITY
    priority is a number. Default priority is set on zero. 
LIMIT-LIST
    limits are set in seconds, bytes or numbers of packets. 
TMPL-LIST
    template list is based on ID, mode, reqid and level. 
ID

is specified by source address, destination address, proto and value of spi.
XFRM_PROTO
    values: esp, ah, comp, route2 or hao. 
MODE

is set as default on transport, but it could be set on tunnel or beet.

LEVEL

is set as default on required and the other choice is use.

UPSPEC

is specified by sport, dport, type and code (NUMBER).

ip xfrm monitor - is used for listing all objects or defined group of them.

The xfrm monitor can monitor the policies for all objects or defined group of them.

History

ip was written by Alexey N. Kuznetsov and added in Linux 2.2. </p>

		<p class=quote>ping</p>
		<p class=terminal>ping(8) - Linux man page
Name
ping, ping6 - send ICMP ECHO_REQUEST to network hosts
Synopsis

ping [ -LRUbdfnqrvVaAB] [ -c count] [ -i interval] [ -l preload] [ -p pattern] [ -s packetsize] [ -t ttl] [ -w deadline] [ -F flowlabel] [ -I interface] [ -M hint] [ -Q tos] [ -S sndbuf] [ -T timestamp option] [ -W timeout] [ hop ...] destination
Description

ping uses the ICMP protocol's mandatory ECHO_REQUEST datagram to elicit an ICMP ECHO_RESPONSE from a host or gateway. ECHO_REQUEST datagrams (''pings'') have an IP and ICMP header, followed by a struct timeval and then an arbitrary number of ''pad'' bytes used to fill out the packet.
Options

-a
    Audible ping. 
-A
    Adaptive ping. Interpacket interval adapts to round-trip time, so that effectively not more than one (or more, if preload is set) unanswered probes present in the network. Minimal interval is 200msec for not super-user. On networks with low rtt this mode is essentially equivalent to flood mode. 
-b
    Allow pinging a broadcast address. 
-B
    Do not allow ping to change source address of probes. The address is bound to one selected when ping starts. 
-c count
    Stop after sending count ECHO_REQUEST packets. With deadline option, ping waits for count ECHO_REPLY packets, until the timeout expires. 
-d
    Set the SO_DEBUG option on the socket being used. Essentially, this socket option is not used by Linux kernel. 
-F flow label
    Allocate and set 20 bit flow label on echo request packets. (Only ping6). If value is zero, kernel allocates random flow label. 
-f
    Flood ping. For every ECHO_REQUEST sent a period ''.'' is printed, while for ever ECHO_REPLY received a backspace is printed. This provides a rapid display of how many packets are being dropped. If interval is not given, it sets interval to zero and outputs packets as fast as they come back or one hundred times per second, whichever is more. Only the super-user may use this option with zero interval. 
-i interval
    Wait interval seconds between sending each packet. The default is to wait for one second between each packet normally, or not to wait in flood mode. Only super-user may set interval to values less 0.2 seconds. 
-I interface address
    Set source address to specified interface address. Argument may be numeric IP address or name of device. When pinging IPv6 link-local address this option is required. 
-l preload
    If preload is specified, ping sends that many packets not waiting for reply. Only the super-user may select preload more than 3. 
-L
    Suppress loopback of multicast packets. This flag only applies if the ping destination is a multicast address. 
-n
    Numeric output only. No attempt will be made to lookup symbolic names for host addresses. 
-p pattern
    You may specify up to 16 ''pad'' bytes to fill out the packet you send. This is useful for diagnosing data-dependent problems in a network. For example, -p ff will cause the sent packet to be filled with all ones. 
-Q tos
    Set Quality of Service -related bits in ICMP datagrams. tos can be either decimal or hex number. Traditionally (RFC1349), these have been interpreted as: 0 for reserved (currently being redefined as congestion control), 1-4 for Type of Service and 5-7 for Precedence. Possible settings for Type of Service are: minimal cost: 0x02, reliability: 0x04, throughput: 0x08, low delay: 0x10. Multiple TOS bits should not be set simultaneously. Possible settings for special Precedence range from priority (0x20) to net control (0xe0). You must be root (CAP_NET_ADMIN capability) to use Critical or higher precedence value. You cannot set bit 0x01 (reserved) unless ECN has been enabled in the kernel. In RFC2474, these fields has been redefined as 8-bit Differentiated Services (DS), consisting of: bits 0-1 of separate data (ECN will be used, here), and bits 2-7 of Differentiated Services Codepoint (DSCP). 
-q
    Quiet output. Nothing is displayed except the summary lines at startup time and when finished. 
-R
    Record route. Includes the RECORD_ROUTE option in the ECHO_REQUEST packet and displays the route buffer on returned packets. Note that the IP header is only large enough for nine such routes. Many hosts ignore or discard this option. 
-r
    Bypass the normal routing tables and send directly to a host on an attached interface. If the host is not on a directly-attached network, an error is returned. This option can be used to ping a local host through an interface that has no route through it provided the option -I is also used. 
-s packetsize
    Specifies the number of data bytes to be sent. The default is 56, which translates into 64 ICMP data bytes when combined with the 8 bytes of ICMP header data. 
-S sndbuf
    Set socket sndbuf. If not specified, it is selected to buffer not more than one packet. 
-t ttl
    Set the IP Time to Live. 
-T timestamp option
    Set special IP timestamp options. timestamp option may be either tsonly (only timestamps), tsandaddr (timestamps and addresses) or tsprespec host1 [host2 [host3 [host4]]] (timestamp prespecified hops). 
-M hint
    Select Path MTU Discovery strategy. hint may be either do (prohibit fragmentation, even local one), want (do PMTU discovery, fragment locally when packet size is large), or dont (do not set DF flag). 
-U
    Print full user-to-user latency (the old behaviour). Normally ping prints network round trip time, which can be different f.e. due to DNS failures. 
-v
    Verbose output. 
-V
    Show version and exit. 
-w deadline
    Specify a timeout, in seconds, before ping exits regardless of how many packets have been sent or received. In this case ping does not stop after count packet are sent, it waits either for deadline expire or until count probes are answered or for some error notification from network. 
-W timeout
    Time to wait for a response, in seconds. The option affects only timeout in absense of any responses, otherwise ping waits for two RTTs.

When using ping for fault isolation, it should first be run on the local host, to verify that the local network interface is up and running. Then, hosts and gateways further and further away should be ''pinged''. Round-trip times and packet loss statistics are computed. If duplicate packets are received, they are not included in the packet loss calculation, although the round trip time of these packets is used in calculating the minimum/average/maximum round-trip time numbers. When the specified number of packets have been sent (and received) or if the program is terminated with a SIGINT, a brief summary is displayed. Shorter current statistics can be obtained without termination of process with signal SIGQUIT.

If ping does not receive any reply packets at all it will exit with code 1. If a packet count and deadline are both specified, and fewer than count packets are received by the time the deadline has arrived, it will also exit with code 1. On other error it exits with code 2. Otherwise it exits with code 0. This makes it possible to use the exit code to see if a host is alive or not.

This program is intended for use in network testing, measurement and management. Because of the load it can impose on the network, it is unwise to use ping during normal operations or from automated scripts.
Icmp Packet Details

An IP header without options is 20 bytes. An ICMP ECHO_REQUEST packet contains an additional 8 bytes worth of ICMP header followed by an arbitrary amount of data. When a packetsize is given, this indicated the size of this extra piece of data (the default is 56). Thus the amount of data received inside of an IP packet of type ICMP ECHO_REPLY will always be 8 bytes more than the requested data space (the ICMP header).

If the data space is at least of size of struct timeval ping uses the beginning bytes of this space to include a timestamp which it uses in the computation of round trip times. If the data space is shorter, no round trip times are given.
Duplicate and Damaged Packets

ping will report duplicate and damaged packets. Duplicate packets should never occur, and seem to be caused by inappropriate link-level retransmissions. Duplicates may occur in many situations and are rarely (if ever) a good sign, although the presence of low levels of duplicates may not always be cause for alarm.

Damaged packets are obviously serious cause for alarm and often indicate broken hardware somewhere in the ping packet's path (in the network or in the hosts).
Trying Different Data Patterns

The (inter)network layer should never treat packets differently depending on the data contained in the data portion. Unfortunately, data-dependent problems have been known to sneak into networks and remain undetected for long periods of time. In many cases the particular pattern that will have problems is something that doesn't have sufficient ''transitions'', such as all ones or all zeros, or a pattern right at the edge, such as almost all zeros. It isn't necessarily enough to specify a data pattern of all zeros (for example) on the command line because the pattern that is of interest is at the data link level, and the relationship between what you type and what the controllers transmit can be complicated.

This means that if you have a data-dependent problem you will probably have to do a lot of testing to find it. If you are lucky, you may manage to find a file that either can't be sent across your network or that takes much longer to transfer than other similar length files. You can then examine this file for repeated patterns that you can test using the -p option of ping.
Ttl Details

The TTL value of an IP packet represents the maximum number of IP routers that the packet can go through before being thrown away. In current practice you can expect each router in the Internet to decrement the TTL field by exactly one.

The TCP/IP specification states that the TTL field for TCP packets should be set to 60, but many systems use smaller values (4.3 BSD uses 30, 4.2 used 15).

The maximum possible value of this field is 255, and most Unix systems set the TTL field of ICMP ECHO_REQUEST packets to 255. This is why you will find you can ''ping'' some hosts, but not reach them with telnet(1) or ftp(1).

In normal operation ping prints the ttl value from the packet it receives. When a remote system receives a ping packet, it can do one of three things with the TTL field in its response:

    Not change it; this is what Berkeley Unix systems did before the 4.3BSD Tahoe release. In this case the TTL value in the received packet will be 255 minus the number of routers in the round-trip path.
    Set it to 255; this is what current Berkeley Unix systems do. In this case the TTL value in the received packet will be 255 minus the number of routers in the path from the remote system to the pinging host.
    Set it to some other value. Some machines use the same value for ICMP packets that they use for TCP packets, for example either 30 or 60. Others may use completely wild values.

Bugs

    Many Hosts and Gateways ignore the RECORD_ROUTE option.
    The maximum IP header length is too small for options like RECORD_ROUTE to be completely useful. There's not much that that can be done about this, however.
    Flood pinging is not recommended in general, and flood pinging the broadcast address should only be done under very controlled conditions.</p>
	
			<p class=quote>wget</p>
		<p class=terminal>wget(1) - Linux man page
Name

Wget - The non-interactive network downloader.
Synopsis

wget [option]... [ URL ]...
Description

GNU Wget is a free utility for non-interactive download of files from the Web. It supports HTTP , HTTPS , and FTP protocols, as well as retrieval through HTTP proxies.

Wget is non-interactive, meaning that it can work in the background, while the user is not logged on. This allows you to start a retrieval and disconnect from the system, letting Wget finish the work. By contrast, most of the Web browsers require constant user's presence, which can be a great hindrance when transferring a lot of data.

Wget can follow links in HTML , XHTML , and CSS pages, to create local versions of remote web sites, fully recreating the directory structure of the original site. This is sometimes referred to as "recursive downloading." While doing that, Wget respects the Robot Exclusion Standard (/robots.txt). Wget can be instructed to convert the links in downloaded files to point at the local files, for offline viewing.

Wget has been designed for robustness over slow or unstable network connections; if a download fails due to a network problem, it will keep retrying until the whole file has been retrieved. If the server supports regetting, it will instruct the server to continue the download from where it left off.
Options

Option Syntax

Since Wget uses GNU getopt to process command-line arguments, every option has a long form along with the short one. Long options are more convenient to remember, but take time to type. You may freely mix different option styles, or specify options after the command-line arguments. Thus you may write:

wget -r --tries=10 http://fly.srk.fer.hr/ -o log

The space between the option accepting an argument and the argument may be omitted. Instead of -o log you can write -olog.

You may put several options that do not require arguments together, like:

wget -drc <URL>

This is completely equivalent to:

wget -d -r -c <URL>

Since the options can be specified after the arguments, you may terminate them with --. So the following will try to download URL -x, reporting failure to log:

wget -o log -- -x

The options that accept comma-separated lists all respect the convention that specifying an empty list clears its value. This can be useful to clear the .wgetrc settings. For instance, if your .wgetrc sets "exclude_directories" to /cgi-bin, the following example will first reset it, and then set it to exclude /~nobody and /~somebody. You can also clear the lists in .wgetrc.

wget -X " -X /~nobody,/~somebody

Most options that do not accept arguments are boolean options, so named because their state can be captured with a yes-or-no ("boolean") variable. For example, --follow-ftp tells Wget to follow FTP links from HTML files and, on the other hand, --no-glob tells it not to perform file globbing on FTP URLs. A boolean option is either affirmative or negative (beginning with --no). All such options share several properties.

Unless stated otherwise, it is assumed that the default behavior is the opposite of what the option accomplishes. For example, the documented existence of --follow-ftp assumes that the default is to not follow FTP links from HTML pages.

Affirmative options can be negated by prepending the --no- to the option name; negative options can be negated by omitting the --no- prefix. This might seem superfluous---if the default for an affirmative option is to not do something, then why provide a way to explicitly turn it off? But the startup file may in fact change the default. For instance, using "follow_ftp = on" in .wgetrc makes Wget follow FTP links by default, and using --no-follow-ftp is the only way to restore the factory default from the command line.

Basic Startup Options

-V
--version
    Display the version of Wget. 
-h
--help
    Print a help message describing all of Wget's command-line options. 
-b
--background
    Go to background immediately after startup. If no output file is specified via the -o, output is redirected to wget-log. 
-e command
--execute command
    Execute command as if it were a part of .wgetrc. A command thus invoked will be executed after the commands in .wgetrc, thus taking precedence over them. If you need to specify more than one wgetrc command, use multiple instances of -e. 

Logging and Input File Options

-o logfile
--output-file=logfile
    Log all messages to logfile. The messages are normally reported to standard error. 
-a logfile
--append-output=logfile
    Append to logfile. This is the same as -o, only it appends to logfile instead of overwriting the old log file. If logfile does not exist, a new file is created. 
-d
--debug
    Turn on debug output, meaning various information important to the developers of Wget if it does not work properly. Your system administrator may have chosen to compile Wget without debug support, in which case -d will not work. Please note that compiling with debug support is always safe---Wget compiled with the debug support will not print any debug info unless requested with -d. 
-q
--quiet
    Turn off Wget's output. 
-v
--verbose
    Turn on verbose output, with all the available data. The default output is verbose. 
-nv
--no-verbose
    Turn off verbose without being completely quiet (use -q for that), which means that error messages and basic information still get printed. 
-i file
--input-file=file
    Read URLs from a local or external file. If - is specified as file, URLs are read from the standard input. (Use ./- to read from a file literally named -.)

    If this function is used, no URLs need be present on the command line. If there are URLs both on the command line and in an input file, those on the command lines will be the first ones to be retrieved. If --force-html is not specified, then file should consist of a series of URLs, one per line.

    However, if you specify --force-html, the document will be regarded as html. In that case you may have problems with relative links, which you can solve either by adding "<base href=" url ">" to the documents or by specifying --base=url on the command line.

    If the file is an external one, the document will be automatically treated as html if the Content-Type matches text/html. Furthermore, the file's location will be implicitly used as base href if none was specified. 
-F
--force-html
    When input is read from a file, force it to be treated as an HTML file. This enables you to retrieve relative links from existing HTML files on your local disk, by adding "<base href=" url ">" to HTML , or using the --base command-line option. 
-B URL
--base= URL
    Resolves relative links using URL as the point of reference, when reading links from an HTML file specified via the -i/--input-file option (together with --force-html, or when the input file was fetched remotely from a server describing it as HTML ). This is equivalent to the presence of a "BASE" tag in the HTML input file, with URL as the value for the "href" attribute.

    For instance, if you specify http://foo/bar/a.html for URL , and Wget reads ../baz/b.html from the input file, it would be resolved to http://foo/baz/b.html. 

Download Options

--bind-address= ADDRESS
    When making client TCP/IP connections, bind to ADDRESS on the local machine. ADDRESS may be specified as a hostname or IP address. This option can be useful if your machine is bound to multiple IPs. 
-t number
--tries=number
    Set number of retries to number. Specify 0 or inf for infinite retrying. The default is to retry 20 times, with the exception of fatal errors like "connection refused" or "not found" (404), which are not retried. 
-O file
--output-document=file
    The documents will not be written to the appropriate files, but all will be concatenated together and written to file. If - is used as file, documents will be printed to standard output, disabling link conversion. (Use ./- to print to a file literally named -.)

    Use of -O is not intended to mean simply "use the name file instead of the one in the URL ;" rather, it is analogous to shell redirection: wget -O file http://foo is intended to work like wget -O - http://foo > file; file will be truncated immediately, and all downloaded content will be written there.

    For this reason, -N (for timestamp-checking) is not supported in combination with -O: since file is always newly created, it will always have a very new timestamp. A warning will be issued if this combination is used.

    Similarly, using -r or -p with -O may not work as you expect: Wget won't just download the first file to file and then download the rest to their normal names: all downloaded content will be placed in file. This was disabled in version 1.11, but has been reinstated (with a warning) in 1.11.2, as there are some cases where this behavior can actually have some use.

    Note that a combination with -k is only permitted when downloading a single document, as in that case it will just convert all relative URIs to external ones; -k makes no sense for multiple URIs when they're all being downloaded to a single file. 
-nc
--no-clobber
    If a file is downloaded more than once in the same directory, Wget's behavior depends on a few options, including -nc. In certain cases, the local file will be clobbered, or overwritten, upon repeated download. In other cases it will be preserved.

    When running Wget without -N, -nc, -r, or -p, downloading the same file in the same directory will result in the original copy of file being preserved and the second copy being named file.1. If that file is downloaded yet again, the third copy will be named file.2, and so on. (This is also the behavior with -nd, even if -r or -p are in effect.) When -nc is specified, this behavior is suppressed, and Wget will refuse to download newer copies of file. Therefore, ""no-clobber"" is actually a misnomer in this mode---it's not clobbering that's prevented (as the numeric suffixes were already preventing clobbering), but rather the multiple version saving that's prevented.

    When running Wget with -r or -p, but without -N, -nd, or -nc, re-downloading a file will result in the new copy simply overwriting the old. Adding -nc will prevent this behavior, instead causing the original version to be preserved and any newer copies on the server to be ignored.

    When running Wget with -N, with or without -r or -p, the decision as to whether or not to download a newer copy of a file depends on the local and remote timestamp and size of the file. -nc may not be specified at the same time as -N.

    Note that when -nc is specified, files with the suffixes .html or .htm will be loaded from the local disk and parsed as if they had been retrieved from the Web. 
-c
--continue
    Continue getting a partially-downloaded file. This is useful when you want to finish up a download started by a previous instance of Wget, or by another program. For instance:

    wget -c ftp://sunsite.doc.ic.ac.uk/ls-lR.Z

    If there is a file named ls-lR.Z in the current directory, Wget will assume that it is the first portion of the remote file, and will ask the server to continue the retrieval from an offset equal to the length of the local file.

    Note that you don't need to specify this option if you just want the current invocation of Wget to retry downloading a file should the connection be lost midway through. This is the default behavior. -c only affects resumption of downloads started prior to this invocation of Wget, and whose local files are still sitting around.

    Without -c, the previous example would just download the remote file to ls-lR.Z.1, leaving the truncated ls-lR.Z file alone.

    Beginning with Wget 1.7, if you use -c on a non-empty file, and it turns out that the server does not support continued downloading, Wget will refuse to start the download from scratch, which would effectively ruin existing contents. If you really want the download to start from scratch, remove the file.

    Also beginning with Wget 1.7, if you use -c on a file which is of equal size as the one on the server, Wget will refuse to download the file and print an explanatory message. The same happens when the file is smaller on the server than locally (presumably because it was changed on the server since your last download attempt)---because "continuing" is not meaningful, no download occurs.

    On the other side of the coin, while using -c, any file that's bigger on the server than locally will be considered an incomplete download and only "(length(remote) - length(local))" bytes will be downloaded and tacked onto the end of the local file. This behavior can be desirable in certain cases---for instance, you can use wget -c to download just the new portion that's been appended to a data collection or log file.

    However, if the file is bigger on the server because it's been changed, as opposed to just appended to, you'll end up with a garbled file. Wget has no way of verifying that the local file is really a valid prefix of the remote file. You need to be especially careful of this when using -c in conjunction with -r, since every file will be considered as an "incomplete download" candidate.

    Another instance where you'll get a garbled file if you try to use -c is if you have a lame HTTP proxy that inserts a "transfer interrupted" string into the local file. In the future a "rollback" option may be added to deal with this case.

    Note that -c only works with FTP servers and with HTTP servers that support the "Range" header. 
--progress=type
    Select the type of the progress indicator you wish to use. Legal indicators are "dot" and "bar".

    The "bar" indicator is used by default. It draws an ASCII progress bar graphics (a.k.a "thermometer" display) indicating the status of retrieval. If the output is not a TTY , the "dot" bar will be used by default.

    Use --progress=dot to switch to the "dot" display. It traces the retrieval by printing dots on the screen, each dot representing a fixed amount of downloaded data.

    When using the dotted retrieval, you may also set the style by specifying the type as dot:style. Different styles assign different meaning to one dot. With the "default" style each dot represents 1K, there are ten dots in a cluster and 50 dots in a line. The "binary" style has a more "computer"-like orientation---8K dots, 16-dots clusters and 48 dots per line (which makes for 384K lines). The "mega" style is suitable for downloading very large files---each dot represents 64K retrieved, there are eight dots in a cluster, and 48 dots on each line (so each line contains 3M).

    Note that you can set the default style using the "progress" command in .wgetrc. That setting may be overridden from the command line. The exception is that, when the output is not a TTY , the "dot" progress will be favored over "bar". To force the bar output, use --progress=bar:force. 
-N
--timestamping
    Turn on time-stamping. 
-S
--server-response
    Print the headers sent by HTTP servers and responses sent by FTP servers. 
--spider
    When invoked with this option, Wget will behave as a Web spider, which means that it will not download the pages, just check that they are there. For example, you can use Wget to check your bookmarks:

    wget --spider --force-html -i bookmarks.html

    This feature needs much more work for Wget to get close to the functionality of real web spiders. 
-T seconds
--timeout=seconds
    Set the network timeout to seconds seconds. This is equivalent to specifying --dns-timeout, --connect-timeout, and --read-timeout, all at the same time.

    When interacting with the network, Wget can check for timeout and abort the operation if it takes too long. This prevents anomalies like hanging reads and infinite connects. The only timeout enabled by default is a 900-second read timeout. Setting a timeout to 0 disables it altogether. Unless you know what you are doing, it is best not to change the default timeout settings.

    All timeout-related options accept decimal values, as well as subsecond values. For example, 0.1 seconds is a legal (though unwise) choice of timeout. Subsecond timeouts are useful for checking server response times or for testing network latency. 
--dns-timeout=seconds
    Set the DNS lookup timeout to seconds seconds. DNS lookups that don't complete within the specified time will fail. By default, there is no timeout on DNS lookups, other than that implemented by system libraries. 
--connect-timeout=seconds
    Set the connect timeout to seconds seconds. TCP connections that take longer to establish will be aborted. By default, there is no connect timeout, other than that implemented by system libraries. 
--read-timeout=seconds
    Set the read (and write) timeout to seconds seconds. The "time" of this timeout refers to idle time: if, at any point in the download, no data is received for more than the specified number of seconds, reading fails and the download is restarted. This option does not directly affect the duration of the entire download.

    Of course, the remote server may choose to terminate the connection sooner than this option requires. The default read timeout is 900 seconds. 
--limit-rate=amount
    Limit the download speed to amount bytes per second. Amount may be expressed in bytes, kilobytes with the k suffix, or megabytes with the m suffix. For example, --limit-rate=20k will limit the retrieval rate to 20KB/s. This is useful when, for whatever reason, you don't want Wget to consume the entire available bandwidth.

    This option allows the use of decimal numbers, usually in conjunction with power suffixes; for example, --limit-rate=2.5k is a legal value.

    Note that Wget implements the limiting by sleeping the appropriate amount of time after a network read that took less time than specified by the rate. Eventually this strategy causes the TCP transfer to slow down to approximately the specified rate. However, it may take some time for this balance to be achieved, so don't be surprised if limiting the rate doesn't work well with very small files. 
-w seconds
--wait=seconds
    Wait the specified number of seconds between the retrievals. Use of this option is recommended, as it lightens the server load by making the requests less frequent. Instead of in seconds, the time can be specified in minutes using the "m" suffix, in hours using "h" suffix, or in days using "d" suffix.

    Specifying a large value for this option is useful if the network or the destination host is down, so that Wget can wait long enough to reasonably expect the network error to be fixed before the retry. The waiting interval specified by this function is influenced by "--random-wait", which see. 
--waitretry=seconds
    If you don't want Wget to wait between every retrieval, but only between retries of failed downloads, you can use this option. Wget will use linear backoff, waiting 1 second after the first failure on a given file, then waiting 2 seconds after the second failure on that file, up to the maximum number of seconds you specify. Therefore, a value of 10 will actually make Wget wait up to (1 + 2 + ... + 10) = 55 seconds per file.

    By default, Wget will assume a value of 10 seconds. 
--random-wait
    Some web sites may perform log analysis to identify retrieval programs such as Wget by looking for statistically significant similarities in the time between requests. This option causes the time between requests to vary between 0.5 and 1.5 * wait seconds, where wait was specified using the --wait option, in order to mask Wget's presence from such analysis.

    A 2001 article in a publication devoted to development on a popular consumer platform provided code to perform this analysis on the fly. Its author suggested blocking at the class C address level to ensure automated retrieval programs were blocked despite changing DHCP-supplied addresses.

    The --random-wait option was inspired by this ill-advised recommendation to block many unrelated users from a web site due to the actions of one. 
--no-proxy
    Don't use proxies, even if the appropriate *_proxy environment variable is defined. 
-Q quota
--quota=quota
    Specify download quota for automatic retrievals. The value can be specified in bytes (default), kilobytes (with k suffix), or megabytes (with m suffix).

    Note that quota will never affect downloading a single file. So if you specify wget -Q10k ftp://wuarchive.wustl.edu/ls-lR.gz, all of the ls-lR.gz will be downloaded. The same goes even when several URLs are specified on the command-line. However, quota is respected when retrieving either recursively, or from an input file. Thus you may safely type wget -Q2m -i sites---download will be aborted when the quota is exceeded.

    Setting quota to 0 or to inf unlimits the download quota. 
--no-dns-cache
    Turn off caching of DNS lookups. Normally, Wget remembers the IP addresses it looked up from DNS so it doesn't have to repeatedly contact the DNS server for the same (typically small) set of hosts it retrieves from. This cache exists in memory only; a new Wget run will contact DNS again.

    However, it has been reported that in some situations it is not desirable to cache host names, even for the duration of a short-running application like Wget. With this option Wget issues a new DNS lookup (more precisely, a new call to "gethostbyname" or "getaddrinfo") each time it makes a new connection. Please note that this option will not affect caching that might be performed by the resolving library or by an external caching layer, such as NSCD .

    If you don't understand exactly what this option does, you probably won't need it. 
--restrict-file-names=modes
    Change which characters found in remote URLs must be escaped during generation of local filenames. Characters that are restricted by this option are escaped, i.e. replaced with %HH, where HH is the hexadecimal number that corresponds to the restricted character. This option may also be used to force all alphabetical cases to be either lower- or uppercase.

    By default, Wget escapes the characters that are not valid or safe as part of file names on your operating system, as well as control characters that are typically unprintable. This option is useful for changing these defaults, perhaps because you are downloading to a non-native partition, or because you want to disable escaping of the control characters, or you want to further restrict characters to only those in the ASCII range of values.

    The modes are a comma-separated set of text values. The acceptable values are unix, windows, nocontrol, ascii, lowercase, and uppercase. The values unix and windows are mutually exclusive (one will override the other), as are lowercase and uppercase. Those last are special cases, as they do not change the set of characters that would be escaped, but rather force local file paths to be converted either to lower- or uppercase.

    When "unix" is specified, Wget escapes the character / and the control characters in the ranges 0--31 and 128--159. This is the default on Unix-like operating systems.

    When "windows" is given, Wget escapes the characters \, |, /, :, ?, ", *, <, >, and the control characters in the ranges 0--31 and 128--159. In addition to this, Wget in Windows mode uses + instead of : to separate host and port in local file names, and uses @ instead of ? to separate the query portion of the file name from the rest. Therefore, a URL that would be saved as www.xemacs.org:4300/search.pl?input=blah in Unix mode would be saved as www.xemacs.org+4300/search.pl@input=blah in Windows mode. This mode is the default on Windows.

    If you specify nocontrol, then the escaping of the control characters is also switched off. This option may make sense when you are downloading URLs whose names contain UTF-8 characters, on a system which can save and display filenames in UTF-8 (some possible byte values used in UTF-8 byte sequences fall in the range of values designated by Wget as "controls").

    The ascii mode is used to specify that any bytes whose values are outside the range of ASCII characters (that is, greater than 127) shall be escaped. This can be useful when saving filenames whose encoding does not match the one used locally. 
-4
--inet4-only
-6
--inet6-only
    Force connecting to IPv4 or IPv6 addresses. With --inet4-only or -4, Wget will only connect to IPv4 hosts, ignoring AAAA records in DNS , and refusing to connect to IPv6 addresses specified in URLs. Conversely, with --inet6-only or -6, Wget will only connect to IPv6 hosts and ignore A records and IPv4 addresses.

    Neither options should be needed normally. By default, an IPv6-aware Wget will use the address family specified by the host's DNS record. If the DNS responds with both IPv4 and IPv6 addresses, Wget will try them in sequence until it finds one it can connect to. (Also see "--prefer-family" option described below.)

    These options can be used to deliberately force the use of IPv4 or IPv6 address families on dual family systems, usually to aid debugging or to deal with broken network configuration. Only one of --inet6-only and --inet4-only may be specified at the same time. Neither option is available in Wget compiled without IPv6 support. 
--prefer-family=none/IPv4/IPv6
    When given a choice of several addresses, connect to the addresses with specified address family first. The address order returned by DNS is used without change by default.

    This avoids spurious errors and connect attempts when accessing hosts that resolve to both IPv6 and IPv4 addresses from IPv4 networks. For example, www.kame.net resolves to 2001:200:0:8002:203:47ff:fea5:3085 and to 203.178.141.194. When the preferred family is "IPv4", the IPv4 address is used first; when the preferred family is "IPv6", the IPv6 address is used first; if the specified value is "none", the address order returned by DNS is used without change.

    Unlike -4 and -6, this option doesn't inhibit access to any address family, it only changes the order in which the addresses are accessed. Also note that the reordering performed by this option is stable---it doesn't affect order of addresses of the same family. That is, the relative order of all IPv4 addresses and of all IPv6 addresses remains intact in all cases. 
--retry-connrefused
    Consider "connection refused" a transient error and try again. Normally Wget gives up on a URL when it is unable to connect to the site because failure to connect is taken as a sign that the server is not running at all and that retries would not help. This option is for mirroring unreliable sites whose servers tend to disappear for short periods of time. 
--user=user
--password=password
    Specify the username user and password password for both FTP and HTTP file retrieval. These parameters can be overridden using the --ftp-user and --ftp-password options for FTP connections and the --http-user and --http-password options for HTTP connections. 
--ask-password
    Prompt for a password for each connection established. Cannot be specified when --password is being used, because they are mutually exclusive. 
--no-iri
    Turn off internationalized URI ( IRI ) support. Use --iri to turn it on. IRI support is activated by default.

    You can set the default state of IRI support using the "iri" command in .wgetrc. That setting may be overridden from the command line. 
--local-encoding=encoding
    Force Wget to use encoding as the default system encoding. That affects how Wget converts URLs specified as arguments from locale to UTF-8 for IRI support.

    Wget use the function "nl_langinfo()" and then the "CHARSET" environment variable to get the locale. If it fails, ASCII is used.

    You can set the default local encoding using the "local_encoding" command in .wgetrc. That setting may be overridden from the command line. 
--remote-encoding=encoding
    Force Wget to use encoding as the default remote server encoding. That affects how Wget converts URIs found in files from remote encoding to UTF-8 during a recursive fetch. This options is only useful for IRI support, for the interpretation of non-ASCII characters.

    For HTTP , remote encoding can be found in HTTP "Content-Type" header and in HTML "Content-Type http-equiv" meta tag.

    You can set the default encoding using the "remoteencoding" command in .wgetrc. That setting may be overridden from the command line. 

Directory Options

-nd
--no-directories
    Do not create a hierarchy of directories when retrieving recursively. With this option turned on, all files will get saved to the current directory, without clobbering (if a name shows up more than once, the filenames will get extensions .n). 
-x
--force-directories
    The opposite of -nd---create a hierarchy of directories, even if one would not have been created otherwise. E.g. wget -x http://fly.srk.fer.hr/robots.txt will save the downloaded file to fly.srk.fer.hr/robots.txt. 
-nH
--no-host-directories
    Disable generation of host-prefixed directories. By default, invoking Wget with -r http://fly.srk.fer.hr/ will create a structure of directories beginning with fly.srk.fer.hr/. This option disables such behavior. 
--protocol-directories
    Use the protocol name as a directory component of local file names. For example, with this option, wget -r http://host will save to http/host/... rather than just to host/.... 
--cut-dirs=number
    Ignore number directory components. This is useful for getting a fine-grained control over the directory where recursive retrieval will be saved.

    Take, for example, the directory at ftp://ftp.xemacs.org/pub/xemacs/. If you retrieve it with -r, it will be saved locally under ftp.xemacs.org/pub/xemacs/. While the -nH option can remove the ftp.xemacs.org/ part, you are still stuck with pub/xemacs. This is where --cut-dirs comes in handy; it makes Wget not "see" number remote directory components. Here are several examples of how --cut-dirs option works.

    No options        -> ftp.xemacs.org/pub/xemacs/
    -nH               -> pub/xemacs/
    -nH --cut-dirs=1  -> xemacs/
    -nH --cut-dirs=2  -> .

    --cut-dirs=1      -> ftp.xemacs.org/xemacs/
    ...

    If you just want to get rid of the directory structure, this option is similar to a combination of -nd and -P. However, unlike -nd, --cut-dirs does not lose with subdirectories---for instance, with -nH --cut-dirs=1, a beta/ subdirectory will be placed to xemacs/beta, as one would expect. 
-P prefix
--directory-prefix=prefix
    Set directory prefix to prefix. The directory prefix is the directory where all other files and subdirectories will be saved to, i.e. the top of the retrieval tree. The default is . (the current directory). 

HTTP Options

--default-page=name
    Use name as the default file name when it isn't known (i.e., for URLs that end in a slash), instead of index.html. 
-E
--adjust-extension
    If a file of type application/xhtml+xml or text/html is downloaded and the URL does not end with the regexp \.[Hh][Tt][Mm][Ll]?, this option will cause the suffix .html to be appended to the local filename. This is useful, for instance, when you're mirroring a remote site that uses .asp pages, but you want the mirrored pages to be viewable on your stock Apache server. Another good use for this is when you're downloading CGI-generated materials. A URL like http://site.com/article.cgi?25 will be saved as article.cgi?25.html.

    Note that filenames changed in this way will be re-downloaded every time you re-mirror a site, because Wget can't tell that the local X.html file corresponds to remote URL X (since it doesn't yet know that the URL produces output of type text/html or application/xhtml+xml. To prevent this re-downloading, you must use -k and -K so that the original version of the file will be saved as X.orig.

    As of version 1.12, Wget will also ensure that any downloaded files of type text/css end in the suffix .css, and the option was renamed from --html-extension, to better reflect its new behavior. The old option name is still acceptable, but should now be considered deprecated.

    At some point in the future, this option may well be expanded to include suffixes for other types of content, including content types that are not parsed by Wget. 
--http-user=user
--http-password=password
    Specify the username user and password password on an HTTP server. According to the type of the challenge, Wget will encode them using either the "basic" (insecure), the "digest", or the Windows "NTLM" authentication scheme.

    Another way to specify username and password is in the URL itself. Either method reveals your password to anyone who bothers to run "ps". To prevent the passwords from being seen, store them in .wgetrc or .netrc, and make sure to protect those files from other users with "chmod". If the passwords are really important, do not leave them lying in those files either---edit the files and delete them after Wget has started the download. 
--no-http-keep-alive
    Turn off the "keep-alive" feature for HTTP downloads. Normally, Wget asks the server to keep the connection open so that, when you download more than one document from the same server, they get transferred over the same TCP connection. This saves time and at the same time reduces the load on the server.

    This option is useful when, for some reason, persistent (keep-alive) connections don't work for you, for example due to a server bug or due to the inability of server-side scripts to cope with the connections. 
--no-cache
    Disable server-side cache. In this case, Wget will send the remote server an appropriate directive (Pragma: no-cache) to get the file from the remote service, rather than returning the cached version. This is especially useful for retrieving and flushing out-of-date documents on proxy servers.

    Caching is allowed by default. 
--no-cookies
    Disable the use of cookies. Cookies are a mechanism for maintaining server-side state. The server sends the client a cookie using the "Set-Cookie" header, and the client responds with the same cookie upon further requests. Since cookies allow the server owners to keep track of visitors and for sites to exchange this information, some consider them a breach of privacy. The default is to use cookies; however, storing cookies is not on by default. 
--load-cookies file
    Load cookies from file before the first HTTP retrieval. file is a textual file in the format originally used by Netscape's cookies.txt file.

    You will typically use this option when mirroring sites that require that you be logged in to access some or all of their content. The login process typically works by the web server issuing an HTTP cookie upon receiving and verifying your credentials. The cookie is then resent by the browser when accessing that part of the site, and so proves your identity.

    Mirroring such a site requires Wget to send the same cookies your browser sends when communicating with the site. This is achieved by --load-cookies---simply point Wget to the location of the cookies.txt file, and it will send the same cookies your browser would send in the same situation. Different browsers keep textual cookie files in different locations: 
    Netscape 4.x. 
    The cookies are in ~/.netscape/cookies.txt. 
    Mozilla and Netscape 6.x. 
    Mozilla's cookie file is also named cookies.txt, located somewhere under ~/.mozilla, in the directory of your profile. The full path usually ends up looking somewhat like ~/.mozilla/default/some-weird-string/cookies.txt. 
    Internet Explorer. 
    You can produce a cookie file Wget can use by using the File menu, Import and Export, Export Cookies. This has been tested with Internet Explorer 5; it is not guaranteed to work with earlier versions. 
    Other browsers. 
    If you are using a different browser to create your cookies, --load-cookies will only work if you can locate or produce a cookie file in the Netscape format that Wget expects. 
    If you cannot use --load-cookies, there might still be an alternative. If your browser supports a "cookie manager", you can use it to view the cookies used when accessing the site you're mirroring. Write down the name and value of the cookie, and manually instruct Wget to send those cookies, bypassing the "official" cookie support:

    wget --no-cookies --header "Cookie: <name>=<value>"

--save-cookies file
    Save cookies to file before exiting. This will not save cookies that have expired or that have no expiry time (so-called "session cookies"), but also see --keep-session-cookies. 
--keep-session-cookies
    When specified, causes --save-cookies to also save session cookies. Session cookies are normally not saved because they are meant to be kept in memory and forgotten when you exit the browser. Saving them is useful on sites that require you to log in or to visit the home page before you can access some pages. With this option, multiple Wget runs are considered a single browser session as far as the site is concerned.

    Since the cookie file format does not normally carry session cookies, Wget marks them with an expiry timestamp of 0. Wget's --load-cookies recognizes those as session cookies, but it might confuse other browsers. Also note that cookies so loaded will be treated as other session cookies, which means that if you want --save-cookies to preserve them again, you must use --keep-session-cookies again. 
--ignore-length
    Unfortunately, some HTTP servers ( CGI programs, to be more precise) send out bogus "Content-Length" headers, which makes Wget go wild, as it thinks not all the document was retrieved. You can spot this syndrome if Wget retries getting the same document again and again, each time claiming that the (otherwise normal) connection has closed on the very same byte.

    With this option, Wget will ignore the "Content-Length" header---as if it never existed. 
--header=header-line
    Send header-line along with the rest of the headers in each HTTP request. The supplied header is sent as-is, which means it must contain name and value separated by colon, and must not contain newlines.

    You may define more than one additional header by specifying --header more than once.

    wget --header='Accept-Charset: iso-8859-2' \
         --header='Accept-Language: hr'        \
           http://fly.srk.fer.hr/

    Specification of an empty string as the header value will clear all previous user-defined headers.

    As of Wget 1.10, this option can be used to override headers otherwise generated automatically. This example instructs Wget to connect to localhost, but to specify foo.bar in the "Host" header:

    wget --header="Host: foo.bar" http://localhost/

    In versions of Wget prior to 1.10 such use of --header caused sending of duplicate headers. 
--max-redirect=number
    Specifies the maximum number of redirections to follow for a resource. The default is 20, which is usually far more than necessary. However, on those occasions where you want to allow more (or fewer), this is the option to use. 
--proxy-user=user
--proxy-password=password
    Specify the username user and password password for authentication on a proxy server. Wget will encode them using the "basic" authentication scheme.

    Security considerations similar to those with --http-password pertain here as well. 
--referer=url
    Include 'Referer: url' header in HTTP request. Useful for retrieving documents with server-side processing that assume they are always being retrieved by interactive web browsers and only come out properly when Referer is set to one of the pages that point to them. 
--save-headers
    Save the headers sent by the HTTP server to the file, preceding the actual contents, with an empty line as the separator. 
-U agent-string
--user-agent=agent-string
    Identify as agent-string to the HTTP server.

    The HTTP protocol allows the clients to identify themselves using a "User-Agent" header field. This enables distinguishing the WWW software, usually for statistical purposes or for tracing of protocol violations. Wget normally identifies as Wget/version, version being the current version number of Wget.

    However, some sites have been known to impose the policy of tailoring the output according to the "User-Agent"-supplied information. While this is not such a bad idea in theory, it has been abused by servers denying information to clients other than (historically) Netscape or, more frequently, Microsoft Internet Explorer. This option allows you to change the "User-Agent" line issued by Wget. Use of this option is discouraged, unless you really know what you are doing.

    Specifying empty user agent with --user-agent="" instructs Wget not to send the "User-Agent" header in HTTP requests. 
--post-data=string
--post-file=file
    Use POST as the method for all HTTP requests and send the specified data in the request body. --post-data sends string as data, whereas --post-file sends the contents of file. Other than that, they work in exactly the same way. In particular, they both expect content of the form "key1=value1&key2=value2", with percent-encoding for special characters; the only difference is that one expects its content as a command-line paramter and the other accepts its content from a file. In particular, --post-file is not for transmitting files as form attachments: those must appear as "key=value" data (with appropriate percent-coding) just like everything else. Wget does not currently support "multipart/form-data" for transmitting POST data; only "application/x-www-form-urlencoded". Only one of --post-data and --post-file should be specified.

    Please be aware that Wget needs to know the size of the POST data in advance. Therefore the argument to "--post-file" must be a regular file; specifying a FIFO or something like /dev/stdin won't work. It's not quite clear how to work around this limitation inherent in HTTP/1 .0. Although HTTP/1 .1 introduces chunked transfer that doesn't require knowing the request length in advance, a client can't use chunked unless it knows it's talking to an HTTP/1 .1 server. And it can't know that until it receives a response, which in turn requires the request to have been completed -- a chicken-and-egg problem.

    Note: if Wget is redirected after the POST request is completed, it will not send the POST data to the redirected URL . This is because URLs that process POST often respond with a redirection to a regular page, which does not desire or accept POST . It is not completely clear that this behavior is optimal; if it doesn't work out, it might be changed in the future.

    This example shows how to log to a server using POST and then proceed to download the desired pages, presumably only accessible to authorized users:

    # Log in to the server.  This can be done only once.
    wget --save-cookies cookies.txt \
         --post-data 'user=foo&password=bar' \
         http://server.com/auth.php

    # Now grab the page or pages we care about.
    wget --load-cookies cookies.txt \
         -p http://server.com/interesting/article.php

    If the server is using session cookies to track user authentication, the above will not work because --save-cookies will not save them (and neither will browsers) and the cookies.txt file will be empty. In that case use --keep-session-cookies along with --save-cookies to force saving of session cookies. 
--content-disposition
    If this is set to on, experimental (not fully-functional) support for "Content-Disposition" headers is enabled. This can currently result in extra round-trips to the server for a "HEAD" request, and is known to suffer from a few bugs, which is why it is not currently enabled by default.

    This option is useful for some file-downloading CGI programs that use "Content-Disposition" headers to describe what the name of a downloaded file should be. 
--auth-no-challenge
    If this option is given, Wget will send Basic HTTP authentication information (plaintext username and password) for all requests, just like Wget 1.10.2 and prior did by default.

    Use of this option is not recommended, and is intended only to support some few obscure servers, which never send HTTP authentication challenges, but accept unsolicited auth info, say, in addition to form-based authentication. 

HTTPS ( SSL/TLS ) Options

To support encrypted HTTP ( HTTPS ) downloads, Wget must be compiled with an external SSL library, currently OpenSSL. If Wget is compiled without SSL support, none of these options are available.
--secure-protocol=protocol
    Choose the secure protocol to be used. Legal values are auto, SSLv2, SSLv3, and TLSv1. If auto is used, the SSL library is given the liberty of choosing the appropriate protocol automatically, which is achieved by sending an SSLv2 greeting and announcing support for SSLv3 and TLSv1. This is the default.

    Specifying SSLv2, SSLv3, or TLSv1 forces the use of the corresponding protocol. This is useful when talking to old and buggy SSL server implementations that make it hard for OpenSSL to choose the correct protocol version. Fortunately, such servers are quite rare. 
--no-check-certificate
    Don't check the server certificate against the available certificate authorities. Also don't require the URL host name to match the common name presented by the certificate.

    As of Wget 1.10, the default is to verify the server's certificate against the recognized certificate authorities, breaking the SSL handshake and aborting the download if the verification fails. Although this provides more secure downloads, it does break interoperability with some sites that worked with previous Wget versions, particularly those using self-signed, expired, or otherwise invalid certificates. This option forces an "insecure" mode of operation that turns the certificate verification errors into warnings and allows you to proceed.

    If you encounter "certificate verification" errors or ones saying that "common name doesn't match requested host name", you can use this option to bypass the verification and proceed with the download. Only use this option if you are otherwise convinced of the site's authenticity, or if you really don't care about the validity of its certificate. It is almost always a bad idea not to check the certificates when transmitting confidential or important data. 
--certificate=file
    Use the client certificate stored in file. This is needed for servers that are configured to require certificates from the clients that connect to them. Normally a certificate is not required and this switch is optional. 
--certificate-type=type
    Specify the type of the client certificate. Legal values are PEM (assumed by default) and DER , also known as ASN1 . 
--private-key=file
    Read the private key from file. This allows you to provide the private key in a file separate from the certificate. 
--private-key-type=type
    Specify the type of the private key. Accepted values are PEM (the default) and DER . 
--ca-certificate=file
    Use file as the file with the bundle of certificate authorities (" CA ") to verify the peers. The certificates must be in PEM format.

    Without this option Wget looks for CA certificates at the system-specified locations, chosen at OpenSSL installation time. 
--ca-directory=directory
    Specifies directory containing CA certificates in PEM format. Each file contains one CA certificate, and the file name is based on a hash value derived from the certificate. This is achieved by processing a certificate directory with the "c_rehash" utility supplied with OpenSSL. Using --ca-directory is more efficient than --ca-certificate when many certificates are installed because it allows Wget to fetch certificates on demand.

    Without this option Wget looks for CA certificates at the system-specified locations, chosen at OpenSSL installation time. 
--random-file=file
    Use file as the source of random data for seeding the pseudo-random number generator on systems without /dev/random.

    On such systems the SSL library needs an external source of randomness to initialize. Randomness may be provided by EGD (see --egd-file below) or read from an external source specified by the user. If this option is not specified, Wget looks for random data in $RANDFILE or, if that is unset, in $HOME/.rnd. If none of those are available, it is likely that SSL encryption will not be usable.

    If you're getting the "Could not seed OpenSSL PRNG ; disabling SSL ." error, you should provide random data using some of the methods described above. 
--egd-file=file
    Use file as the EGD socket. EGD stands for Entropy Gathering Daemon, a user-space program that collects data from various unpredictable system sources and makes it available to other programs that might need it. Encryption software, such as the SSL library, needs sources of non-repeating randomness to seed the random number generator used to produce cryptographically strong keys.

    OpenSSL allows the user to specify his own source of entropy using the "RAND_FILE" environment variable. If this variable is unset, or if the specified file does not produce enough randomness, OpenSSL will read random data from EGD socket specified using this option.

    If this option is not specified (and the equivalent startup command is not used), EGD is never contacted. EGD is not needed on modern Unix systems that support /dev/random. 

FTP Options

--ftp-user=user
--ftp-password=password
    Specify the username user and password password on an FTP server. Without this, or the corresponding startup option, the password defaults to -wget@, normally used for anonymous FTP .

    Another way to specify username and password is in the URL itself. Either method reveals your password to anyone who bothers to run "ps". To prevent the passwords from being seen, store them in .wgetrc or .netrc, and make sure to protect those files from other users with "chmod". If the passwords are really important, do not leave them lying in those files either---edit the files and delete them after Wget has started the download. 
--no-remove-listing
    Don't remove the temporary .listing files generated by FTP retrievals. Normally, these files contain the raw directory listings received from FTP servers. Not removing them can be useful for debugging purposes, or when you want to be able to easily check on the contents of remote server directories (e.g. to verify that a mirror you're running is complete).

    Note that even though Wget writes to a known filename for this file, this is not a security hole in the scenario of a user making .listing a symbolic link to /etc/passwd or something and asking "root" to run Wget in his or her directory. Depending on the options used, either Wget will refuse to write to .listing, making the globbing/recursion/time-stamping operation fail, or the symbolic link will be deleted and replaced with the actual .listing file, or the listing will be written to a .listing.number file.

    Even though this situation isn't a problem, though, "root" should never run Wget in a non-trusted user's directory. A user could do something as simple as linking index.html to /etc/passwd and asking "root" to run Wget with -N or -r so the file will be overwritten. 
--no-glob
    Turn off FTP globbing. Globbing refers to the use of shell-like special characters (wildcards), like *, ?, [ and ] to retrieve more than one file from the same directory at once, like:

    wget ftp://gnjilux.srk.fer.hr/*.msg

    By default, globbing will be turned on if the URL contains a globbing character. This option may be used to turn globbing on or off permanently.

    You may have to quote the URL to protect it from being expanded by your shell. Globbing makes Wget look for a directory listing, which is system-specific. This is why it currently works only with Unix FTP servers (and the ones emulating Unix "ls" output). 
--no-passive-ftp
    Disable the use of the passive FTP transfer mode. Passive FTP mandates that the client connect to the server to establish the data connection rather than the other way around.

    If the machine is connected to the Internet directly, both passive and active FTP should work equally well. Behind most firewall and NAT configurations passive FTP has a better chance of working. However, in some rare firewall configurations, active FTP actually works when passive FTP doesn't. If you suspect this to be the case, use this option, or set "passive_ftp=off" in your init file. 
--retr-symlinks
    Usually, when retrieving FTP directories recursively and a symbolic link is encountered, the linked-to file is not downloaded. Instead, a matching symbolic link is created on the local filesystem. The pointed-to file will not be downloaded unless this recursive retrieval would have encountered it separately and downloaded it anyway.

    When --retr-symlinks is specified, however, symbolic links are traversed and the pointed-to files are retrieved. At this time, this option does not cause Wget to traverse symlinks to directories and recurse through them, but in the future it should be enhanced to do this.

    Note that when retrieving a file (not a directory) because it was specified on the command-line, rather than because it was recursed to, this option has no effect. Symbolic links are always traversed in this case. 

Recursive Retrieval Options

-r
--recursive
    Turn on recursive retrieving. 
-l depth
--level=depth
    Specify recursion maximum depth level depth. The default maximum depth is 5. 
--delete-after
    This option tells Wget to delete every single file it downloads, after having done so. It is useful for pre-fetching popular pages through a proxy, e.g.:

    wget -r -nd --delete-after http://whatever.com/~popular/page/

    The -r option is to retrieve recursively, and -nd to not create directories.

    Note that --delete-after deletes files on the local machine. It does not issue the DELE command to remote FTP sites, for instance. Also note that when --delete-after is specified, --convert-links is ignored, so .orig files are simply not created in the first place. 
-k
--convert-links
    After the download is complete, convert the links in the document to make them suitable for local viewing. This affects not only the visible hyperlinks, but any part of the document that links to external content, such as embedded images, links to style sheets, hyperlinks to non-HTML content, etc.

    Each link will be changed in one of the two ways: 
    â€¢ The links to files that have been downloaded by Wget will be changed to refer to the file they point to as a relative link. 
    Example: if the downloaded file /foo/doc.html links to /bar/img.gif, also downloaded, then the link in doc.html will be modified to point to ../bar/img.gif. This kind of transformation works reliably for arbitrary combinations of directories. 
    â€¢ The links to files that have not been downloaded by Wget will be changed to include host name and absolute path of the location they point to. 
    Example: if the downloaded file /foo/doc.html links to /bar/img.gif (or to ../bar/img.gif), then the link in doc.html will be modified to point to http://hostname/bar/img.gif. 
    Because of this, local browsing works reliably: if a linked file was downloaded, the link will refer to its local name; if it was not downloaded, the link will refer to its full Internet address rather than presenting a broken link. The fact that the former links are converted to relative links ensures that you can move the downloaded hierarchy to another directory.

    Note that only at the end of the download can Wget know which links have been downloaded. Because of that, the work done by -k will be performed at the end of all the downloads. 
-K
--backup-converted
    When converting a file, back up the original version with a .orig suffix. Affects the behavior of -N. 
-m
--mirror
    Turn on options suitable for mirroring. This option turns on recursion and time-stamping, sets infinite recursion depth and keeps FTP directory listings. It is currently equivalent to -r -N -l inf --no-remove-listing. 
-p
--page-requisites
    This option causes Wget to download all the files that are necessary to properly display a given HTML page. This includes such things as inlined images, sounds, and referenced stylesheets.

    Ordinarily, when downloading a single HTML page, any requisite documents that may be needed to display it properly are not downloaded. Using -r together with -l can help, but since Wget does not ordinarily distinguish between external and inlined documents, one is generally left with "leaf documents" that are missing their requisites.

    For instance, say document 1.html contains an "<IMG>" tag referencing 1.gif and an "<A>" tag pointing to external document 2.html. Say that 2.html is similar but that its image is 2.gif and it links to 3.html. Say this continues up to some arbitrarily high number.

    If one executes the command:

    wget -r -l 2 http://<site>/1.html

    then 1.html, 1.gif, 2.html, 2.gif, and 3.html will be downloaded. As you can see, 3.html is without its requisite 3.gif because Wget is simply counting the number of hops (up to 2) away from 1.html in order to determine where to stop the recursion. However, with this command:

    wget -r -l 2 -p http://<site>/1.html

    all the above files and 3.html's requisite 3.gif will be downloaded. Similarly,

    wget -r -l 1 -p http://<site>/1.html

    will cause 1.html, 1.gif, 2.html, and 2.gif to be downloaded. One might think that:

    wget -r -l 0 -p http://<site>/1.html

    would download just 1.html and 1.gif, but unfortunately this is not the case, because -l 0 is equivalent to -l inf---that is, infinite recursion. To download a single HTML page (or a handful of them, all specified on the command-line or in a -i URL input file) and its (or their) requisites, simply leave off -r and -l:

    wget -p http://<site>/1.html

    Note that Wget will behave as if -r had been specified, but only that single page and its requisites will be downloaded. Links from that page to external documents will not be followed. Actually, to download a single page and all its requisites (even if they exist on separate websites), and make sure the lot displays properly locally, this author likes to use a few options in addition to -p:

    wget -E -H -k -K -p http://<site>/<document>

    To finish off this topic, it's worth knowing that Wget's idea of an external document link is any URL specified in an "<A>" tag, an "<AREA>" tag, or a "<LINK>" tag other than "<LINK REL="stylesheet">". 
--strict-comments
    Turn on strict parsing of HTML comments. The default is to terminate comments at the first occurrence of -->.

    According to specifications, HTML comments are expressed as SGML declarations. Declaration is special markup that begins with <! and ends with >, such as <!DOCTYPE ...>, that may contain comments between a pair of -- delimiters. HTML comments are "empty declarations", SGML declarations without any non-comment text. Therefore, <!--foo--> is a valid comment, and so is <!--one-- --two-->, but <!--1--2--> is not.

    On the other hand, most HTML writers don't perceive comments as anything other than text delimited with <!-- and -->, which is not quite the same. For example, something like <!------------> works as a valid comment as long as the number of dashes is a multiple of four (!). If not, the comment technically lasts until the next --, which may be at the other end of the document. Because of this, many popular browsers completely ignore the specification and implement what users have come to expect: comments delimited with <!-- and -->.

    Until version 1.9, Wget interpreted comments strictly, which resulted in missing links in many web pages that displayed fine in browsers, but had the misfortune of containing non-compliant comments. Beginning with version 1.9, Wget has joined the ranks of clients that implements "naive" comments, terminating each comment at the first occurrence of -->.

    If, for whatever reason, you want strict comment parsing, use this option to turn it on. 

Recursive Accept/Reject Options

-A acclist --accept acclist
-R rejlist --reject rejlist
    Specify comma-separated lists of file name suffixes or patterns to accept or reject. Note that if any of the wildcard characters, *, ?, [ or ], appear in an element of acclist or rejlist, it will be treated as a pattern, rather than a suffix. 
-D domain-list
--domains=domain-list
    Set domains to be followed. domain-list is a comma-separated list of domains. Note that it does not turn on -H. 
--exclude-domains domain-list
    Specify the domains that are not to be followed.. 
--follow-ftp
    Follow FTP links from HTML documents. Without this option, Wget will ignore all the FTP links. 
--follow-tags=list
    Wget has an internal table of HTML tag / attribute pairs that it considers when looking for linked documents during a recursive retrieval. If a user wants only a subset of those tags to be considered, however, he or she should be specify such tags in a comma-separated list with this option. 
--ignore-tags=list
    This is the opposite of the --follow-tags option. To skip certain HTML tags when recursively looking for documents to download, specify them in a comma-separated list.

    In the past, this option was the best bet for downloading a single page and its requisites, using a command-line like:

    wget --ignore-tags=a,area -H -k -K -r http://<site>/<document>

    However, the author of this option came across a page with tags like "<LINK REL="home" HREF="/">" and came to the realization that specifying tags to ignore was not enough. One can't just tell Wget to ignore "<LINK>", because then stylesheets will not be downloaded. Now the best bet for downloading a single page and its requisites is the dedicated --page-requisites option. 
--ignore-case
    Ignore case when matching files and directories. This influences the behavior of -R, -A, -I, and -X options, as well as globbing implemented when downloading from FTP sites. For example, with this option, -A *.txt will match file1.txt, but also file2.TXT, file3.TxT, and so on. 
-H
--span-hosts
    Enable spanning across hosts when doing recursive retrieving. 
-L
--relative
    Follow relative links only. Useful for retrieving a specific home page without any distractions, not even those from the same hosts. 
-I list
--include-directories=list
    Specify a comma-separated list of directories you wish to follow when downloading. Elements of list may contain wildcards. 
-X list
--exclude-directories=list
    Specify a comma-separated list of directories you wish to exclude from download. Elements of list may contain wildcards. 
-np
--no-parent
    Do not ever ascend to the parent directory when retrieving recursively. This is a useful option, since it guarantees that only the files below a certain hierarchy will be downloaded. 

Exit Status

Wget may return one of several error codes if it encounters problems.

0

No problems occurred.

1

Generic error code.

2

Parse error---for instance, when parsing command-line options, the .wgetrc or .netrc...

3

File I/O error.

4

Network failure.

5

SSL verification failure.

6

Username/password authentication failure.

7

Protocol errors.

8

Server issued an error response.
With the exceptions of 0 and 1, the lower-numbered exit codes take precedence over higher-numbered ones, when multiple types of errors are encountered.

In versions of Wget prior to 1.12, Wget's exit status tended to be unhelpful and inconsistent. Recursive downloads would virtually always return 0 (success), regardless of any issues encountered, and non-recursive fetches only returned the status corresponding to the most recently-attempted download.

Files

/etc/wgetrc

    Default location of the global startup file. 
.wgetrc
    User startup file. 

Bugs

You are welcome to submit bug reports via the GNU Wget bug tracker (see <http://wget.addictivecode.org/BugTracker>).

Before actually submitting a bug report, please try to follow a few simple guidelines.

1.

Please try to ascertain that the behavior you see really is a bug. If Wget crashes, it's a bug. If Wget does not behave as documented, it's a bug. If things work strange, but you are not sure about the way they are supposed to work, it might well be a bug, but you might want to double-check the documentation and the mailing lists.

2.

Try to repeat the bug in as simple circumstances as possible. E.g. if Wget crashes while downloading wget -rl0 -kKE -t5 --no-proxy http://yoyodyne.com -o /tmp/log, you should try to see if the crash is repeatable, and if will occur with a simpler set of options. You might even try to start the download at the page where the crash occurred to see if that page somehow triggered the crash.
Also, while I will probably be interested to know the contents of your .wgetrc file, just dumping it into the debug message is probably a bad idea. Instead, you should first try to see if the bug repeats with .wgetrc moved out of the way. Only if it turns out that .wgetrc settings affect the bug, mail me the relevant parts of the file.
3.

Please start Wget with -d option and send us the resulting output (or relevant parts thereof). If Wget was compiled without debug support, recompile it---it is much easier to trace bugs with debug support on.
Note: please make sure to remove any potentially sensitive information from the debug log before sending it to the bug address. The "-d" won't go out of its way to collect sensitive information, but the log will contain a fairly complete transcript of Wget's communication with the server, which may include passwords and pieces of downloaded data. Since the bug address is publically archived, you may assume that all bug reports are visible to the public.
4.

If Wget has crashed, try to run it in a debugger, e.g. "gdb `which wget` core" and type "where" to get the backtrace. This may not work if the system administrator has disabled core files, but it is safe to try.

See Also

This is not the complete manual for GNU Wget. For more complete information, including more detailed explanations of some of the options, and a number of commands available for use with .wgetrc files and the -e option, see the GNU Info entry for wget.
Author

Originally written by Hrvoje Niksic <hniksic@xemacs.org>. Currently maintained by Micah Cowan <micah@cowan.name>. </p>

		<footer>
			<p>Sergey Holoshchuk Kozaev // MP04 - LMSGI</p>
		</footer>
	</body>

</html>